{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIU bloc 4 : \"Bases de données : création de schémas et normalisation\" : TP sur les algorithmes de jointure\n",
    "====================================================\n",
    "\n",
    "Dans ce TP, on va s'intéresser **aux algorithmes de jointures**, c'est-à-dire aux algorithmes exécutés par les moteurs des SGBDs quand ils traduisent des requêtes comme la suivante :\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM table1 JOIN table2 ON table1.attr1 == table2.attr2\n",
    "```\n",
    "\n",
    "Il existe plusieurs algorithmes de jointure et l'optimiseur de requêtes du SGBD va tâcher de choisir le _meilleur_, vis-à-vis de statistiques sur les données et surtout des **index** disponibles sur les tables.\n",
    "Le but du TP est ainsi de comprendre ces algorithmes fondamentaux et de les comparer entre eux puis de les comparer face à deux de SQLite 3.\n",
    "\n",
    "**Remarque** la comparaison de performance (_benchmark_) est un exercice complexe car de nombreux paramètres très différents contribuent à la performance finale (matériel, OS, I/O disques ou d'affichage, efficacité de la compilation/interprétation du langage de programmation, caches, temps d'initialisation etc.).\n",
    "\n",
    "Implanter les algorithmes classiques de jointure en Python\n",
    "-----------------------------------------------------------\n",
    "\n",
    "Le fichier [`join_algorithms.py`](join_algorithms.py) contient le squelette à remplir pour les trois algorithmes, à savoir _nested loop_, _hash join_ et _merge join_. Ces algorithmes font la même chose et ont la même signature `def algo(table1, attr1, table2, attr2):` :\n",
    "\n",
    "* `table1` et `table2` sont des listes (Python) de tuples (Python). Il n'y a pas de garanties d'ordre sur ces listes;\n",
    "* `attr1` (resp. `attr2`) est _l'indice_ (entier) de l'attribut de `table1` (resp. de `tablee`) sur lequel on fait la jointure;\n",
    "* ces algorithmes retournent tous une liste de tuples, comme l'aurait fait la requête SQL.\n",
    "\n",
    "\n",
    "Le fichier [`join_algorithms_test.py`](join_algorithms_test.py) donne un exemple d'entrées et de résultats attendus.\n",
    "\n",
    "**EXERCICE** : compléter la fonction `join_nested_loop` et tester votre implantation avec `pytest-3` et les tests fournis.\n",
    "\n",
    "**EXERCICE** : compléter la fonction `join_hash` et tester votre implantation avec `pytest-3` et les tests fournis.\n",
    "\n",
    "**EXERCICE (POUR ALLER PLUS LOIN)** : compléter la fonction `join_merge` et tester votre implantations avec `pytest-3` et les tests fournis. Vous n'êtes pas obligé de faire cet exercice pour passer à la suite.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Un module pour illustrer les principaux alogorithmes de jointure en Python\n",
    "    On ne gèrera ici le cas nominal d'utilisation des fonctions où\n",
    "    attr1 et attr2 sont LES INDEX EXISTANT des attributs\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from random import randrange\n",
    "from timeit import timeit\n",
    "from collections import defaultdict\n",
    "# voir https://docs.python.org/3.6/library/collections.html#collections.defaultdict\n",
    "# est utile pour le hash join en initialisant à la liste vide\n",
    "\n",
    "\n",
    "\n",
    "def join_nested_loop(table1, attr1, table2, attr2):\n",
    "    \"\"\"L'algorithme naif de jointure par deux boucles imbriquées :\n",
    "       pour chaque tuple de tabl1, on va lire toute la table2\n",
    "       et produire un résultat à chaque fois que tup1[attr1] == tup2[attr2]\n",
    "       https://en.wikipedia.org/wiki/Nested_loop_join\n",
    "\n",
    "       CONTRAT: pas de préconditions sur les entrées\"\"\"\n",
    "    return [t1 + t2 for t1 in table1 for t2 in table2 if t1[attr1] == t2[attr2]]\n",
    "\n",
    "\n",
    "def join_hash(table1, attr1, table2, attr2):\n",
    "    \"\"\"L'algorithme de jointure en deux étapes :\n",
    "          1. construction d'un index sur table1 via un dictionnaire :\n",
    "             à chaque valeur de attr1, on associe **la liste** des index des\n",
    "             tuples de table1 qui ont cette valeur\n",
    "          2. scan de table2 :\n",
    "             pour chaque tuple, on va chercher AVEC L'INDEX la liste\n",
    "             des tuples de table1 qui on la valeur de attr2 pour attr1\n",
    "             On fait attention aux collisions de hash et on ajoute les\n",
    "             tuples au résultat.\n",
    "\n",
    "        https://en.wikipedia.org/wiki/Hash_join#Classic_hash_join\n",
    "\n",
    "        CONTRAT: pas de préconditions sur les entrées\"\"\"\n",
    "    res = []\n",
    "    index = dict()\n",
    "    for k, t1 in enumerate(table1):\n",
    "        val = t1[attr1]\n",
    "        #pour gérer les collisions de hash\n",
    "        #on stocke dans index[val] une liste de listes dont le premier élément est t1[attr1]\n",
    "        #et dont les éléments suivants sont les index des tuples de table1 qui ont même\n",
    "        #même valeur val pour l'attribut d'index attr1\n",
    "        if val in index:\n",
    "            for l in  index[val]:\n",
    "                if l[0] == val:\n",
    "                    l.append(k)\n",
    "                    break\n",
    "        else:\n",
    "            index[val] = [[val, k]]\n",
    "    for t2 in table2:\n",
    "        val = t2[attr2]\n",
    "        if val in index:\n",
    "            for l in index[val]:\n",
    "                if l[0] == val:\n",
    "                    for k in l[1:]:\n",
    "                        res.append(table1[k] + t2)\n",
    "    return res\n",
    "\n",
    "def join_hash2(table1, attr1, table2, attr2):\n",
    "    \"\"\"L'algorithme de jointure en deux étapes :\n",
    "          1. construction d'un index sur table1 via un dictionnaire :\n",
    "             à chaque valeur de attr1, on associe **la liste** des index des\n",
    "             tuples de table1 qui ont cette valeur\n",
    "          2. scan de table2 :\n",
    "             pour chaque tuple, on va chercher AVEC L'INDEX la liste\n",
    "             des tuples de table1 qui on la valeur de attr2 pour attr1\n",
    "             On fait attention aux collisions de hash et on ajoute les\n",
    "             tuples au résultat.\n",
    "\n",
    "        https://en.wikipedia.org/wiki/Hash_join#Classic_hash_join\n",
    "\n",
    "        CONTRAT: pas de préconditions sur les entrées\"\"\"\n",
    "    # en fait, comme les dictionnaires Python utilisent des tables de hash\n",
    "    # il suffit d'un dictionnaire qui sert d'index (un peu comme pour le\n",
    "    # décorateur memoize) pour implémenter cet algorithme\n",
    "    #hash_table = defaultdict(list)\n",
    "    #pass\n",
    "    res = []\n",
    "    index = dict()\n",
    "    for k, t1 in enumerate(table1):\n",
    "        val = t1[attr1]\n",
    "        #pour gérer les collisions de hash\n",
    "        #on stocke dans index[val] une liste de listes dont le premier élément est t1[attr1]\n",
    "        #et dont les éléments suivants sont les index des tuples de table1 qui ont même\n",
    "        #même valeur val pour l'attribut d'index attr1\n",
    "        if val in index:\n",
    "            index[val].append(k)\n",
    "        else:\n",
    "            index[val] = [k]\n",
    "    for t2 in table2:\n",
    "        val = t2[attr2]\n",
    "        if val in index:\n",
    "            for k in index[val]:\n",
    "                res.append(table1[k] + t2)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def join_merge(table1, attr1, table2, attr2):\n",
    "    \"\"\"L'algorithme de jointure sort-merge qui s'appuie sur des tables SUPPOSEES TRIEES\n",
    "       https://en.wikipedia.org/wiki/Sort-merge_join\n",
    "       Son principe est assez similaire à l'étape \"merge\" du merge sort\n",
    "       https://en.wikipedia.org/wiki/Merge_sort\n",
    "\n",
    "       On avance en // sur table1 et table2 avec 2 index ind1 et ind2\n",
    "        - si table1[ind1][attr1] est avant table2[ind2][attr2]\n",
    "          on incrémente ind1\n",
    "        - si table2[ind2][attr2] est avant table1[ind1][attr1]\n",
    "          on incrémente ind2\n",
    "        - si les tuples sur lesquels on se trouve respectent la condition\n",
    "                table1[ind1][attr1] == table2[ind2][attr2]\n",
    "           alors avec une boucle locale, on va chercher tous les tuples\n",
    "           de table2 satisfont la condition et ajouter au résultat.\n",
    "           ensuite on incrémente ind1\n",
    "\n",
    "       CONTRAT : le trie des entrées est à la charge des utilisateurs,\n",
    "                 le comportement n'est pas garanti sinon\"\"\"\n",
    "\n",
    "    res = []\n",
    "    n1 = len(table1)\n",
    "    n2 = len(table2)\n",
    "    ind1 = ind2 = 0\n",
    "    while ind1 < n1 and ind2 < n2:\n",
    "        t1, t2 = table1[ind1], table2[ind2]\n",
    "        if t1[attr1] < t2[attr2]:\n",
    "            ind1 += 1\n",
    "        elif t1[attr1] > t2[attr2]:\n",
    "            ind2 += 1\n",
    "        else:\n",
    "            ind3 = ind2\n",
    "            while ind3 < n2 and table2[ind3][attr2] == t1[attr1]:\n",
    "                res.append(t1 + table2[ind3])\n",
    "                ind3 += 1\n",
    "            ind1 += 1\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "fjunier@fjunier:~/Git/DIU-Junier/bloc4/BDD_Conception_Normalisation/sandbox$ pytest-3 -v\n",
    "============================= test session starts ==============================\n",
    "platform linux -- Python 3.6.9, pytest-3.3.2, py-1.5.2, pluggy-0.6.0 -- /usr/bin/python3\n",
    "cachedir: .cache\n",
    "rootdir: /home/fjunier/Git/DIU-Junier/bloc4/BDD_Conception_Normalisation/sandbox, inifile:\n",
    "plugins: Faker-4.1.0\n",
    "collected 3 items\n",
    "\n",
    "join_algorithms_test.py::test_join_nested_loop PASSED                    [ 33%]\n",
    "join_algorithms_test.py::test_join_hash PASSED                           [ 66%]\n",
    "join_algorithms_test.py::test_join_merge PASSED                          [100%]\n",
    "\n",
    "=========================== 3 passed in 0.07 seconds ===========================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Comparer algorithmes implantés en Python\n",
    "----------------------------------------\n",
    "\n",
    "On peut maintenant comparer la performance des algorithmes avec la fonction fournie `benchmark`.Pour la fonction `join_merge` on compte séparément le temps pris pour le tris des tables.\n",
    "En effet, cette étape peut-être _amortie_ car elle est utile pour d'autre opérations que la jointure, comme les clauses `ORDER BY` ou `GROUP BY`.\n",
    "\n",
    "**EXERCICE** : comprendre ce que fait la fonction `benchmark` (vous pouvez ajouter des commentaires par exempl)e avant de l'exécuter. \n",
    "\n",
    "Avec les paramètres par défaut de `benchmark`, on obtient les résultats suivants sur une machine portable (Dual Core Intel i7-5600U CPU @ 2.60GHz, 8GB RAM).\n",
    "\n",
    "```\n",
    "Temps pour une exécution de join_nested_loop : 4.8441446340002585\n",
    "Temps pour une exécution de join_hash        : 0.1884105869976338\n",
    "Temps pour une exécution des tris            : 0.018489982991013676\n",
    "Temps pour une exécution de join_merge       : 0.3174076739960583\n",
    "```\n",
    "\n",
    "**EXERCICE (POUR ALLER PLUS LOIN)** : jouer avec les paramètres pour trouver un cas qui soit défavorable à `join_hash` mais favorable à `join_merge`. Sans tenir compte du temps de tri, on peut trouver des cas avec un facteur 10x en faveur de `join_merge`. _Indice_ : remarquez que les rôles de `table1` et `table2` sont asymétriques faire en sorte de passer du temps dans l'étape de construction d'index de `join_hash`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(size_1=1000, nb_val_1=100,\n",
    "              size_2=1000, nb_val_2=100,\n",
    "              nb_repeat=100,\n",
    "              bench_loop=True, bench_hash=True, bench_merge=True):\n",
    "    \"\"\"Compare les différentes implémentations\"\"\"\n",
    "\n",
    "    #échantillon 1  \n",
    "    sample_1 = [(i, randrange(nb_val_1)) for i in range(size_1)]\n",
    "    #échantillon 2\n",
    "    sample_2 = [(randrange(nb_val_2), 'A'+str(j)) for j in range(size_2)]\n",
    "\n",
    "    if bench_loop:\n",
    "        #temps d'exécution moyen sur nb_repeat tours de boucles \n",
    "        #pour l'exécution de la jointure avec join_nested_loop\n",
    "        #sur la correspondance des attibuts d'index 1 de sample_1 et 0 de sample_2 \n",
    "        #le domaine de sample_1[0] est  randrange(nb_val_1)\n",
    "        #le domaine de sample_2[0] est  randrange(nb_val_2)\n",
    "        time_loop = timeit(lambda: join_nested_loop(sample_1, 1, sample_2, 0), number=nb_repeat)\n",
    "        print('Temps pour une exécution de join_nested_loop : ' + str(time_loop))\n",
    "\n",
    "    if bench_hash:\n",
    "        #temps d'exécution moyen sur nb_repeat tours de boucles \n",
    "        #pour l'exécution de la jointure avec join_hash\n",
    "        time_hash = timeit(lambda: join_hash(sample_1, 1, sample_2, 0), number=nb_repeat)\n",
    "        print('Temps pour une exécution de join_hash        : ' + str(time_hash))\n",
    "\n",
    "    if bench_merge:\n",
    "        #temps d'exécution du tri de sample_1\n",
    "        time_sort = timeit(lambda: sample_1.sort(key=lambda x: x[1]), number=nb_repeat)\n",
    "        #temps d'exécution du tri de sample_2\n",
    "        time_sort += timeit(lambda: sample_2.sort(key=lambda x: x[0]), number=nb_repeat)\n",
    "        #temps d'exécution moyen sur nb_repeat tours de boucles \n",
    "        #pour l'exécution de la jointure avec join_merge\n",
    "        time_merge = timeit(lambda: join_merge(sample_1, 1, sample_2, 0), number=nb_repeat)\n",
    "        print('Temps pour une exécution des tris            : ' + str(time_sort))\n",
    "        print('Temps pour une exécution de join_merge       : ' + str(time_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour une exécution de join_nested_loop : 6.236525565000193\n",
      "Temps pour une exécution de join_hash        : 0.20081901899902732\n",
      "Temps pour une exécution des tris            : 0.021161122997000348\n",
      "Temps pour une exécution de join_merge       : 0.3073058990012214\n"
     ]
    }
   ],
   "source": [
    "benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Test avec join_hash première version\n",
    "In [2]: benchmark()\n",
    "Temps pour une exécution de join_nested_loop : 6.241203233000078\n",
    "Temps pour une exécution de join_hash        : 0.16518595900015498\n",
    "Temps pour une exécution des tris            : 0.020488148000140427\n",
    "Temps pour une exécution de join_merge       : 0.3143313499999749\n",
    "```\n",
    "\n",
    "```\n",
    "Test avec join_hash seconde version\n",
    "In [4]: benchmark()\n",
    "Temps pour une exécution de join_nested_loop : 6.2457434129999\n",
    "Temps pour une exécution de join_hash        : 0.162923426000134\n",
    "Temps pour une exécution des tris            : 0.02036094400023103\n",
    "Temps pour une exécution de join_merge       : 0.2916302970002107\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de complexité \n",
    "\n",
    "Soit `m` le nombre de lignes dans la jointure. Ce nombre est majoré par `max(nb_val_1, nb_val_2)`\n",
    "\n",
    "* Complexité de `join_nested_loop` : `O(size_1 * size_2)`\n",
    "* Complexité de `join_hash` :  `O(size_1 * n + size_2 * n)` (majoration grossière) où `n` est le nombre maximal de tuple de l'échantillon `sample_1` qui ont la même valeur sur l'attribut 1 (longueur maximale d'une liste d'indexs stockée dans l'index/table de hash) : la valeur de `n` est d'autant plus grande que le domaine de `sample_1[1]` de taille `nb_val_1` est grand. On peut donc prendre comme majorant `O(size_1 * nb_val_1 + size_2 * nb_val_1)` \n",
    "* Complexité de `join_merge` (sans compter le tri des tables) :  `O(size_1 + size_2 + size_2 * m)` où `m` est le nombre de lignes dans la jointure.\n",
    "\n",
    "\n",
    "Si `size_1 == size_2` et `nb_val_1 == nb_val_2`, comme les valeurs sont choisies selon une loi uniforme dans `[0;nb_val_1]`,   les complexités de `join_hash` et `join_merge` doivent avoir  des majorants du même ordre de grandeur, qui vont différer par une constante. On remarque que `join_hash` est plus rapide : les tests d'égalité sont remplacés par des calculs de `hash` dans la phase de construction de l'index puis dans la phase de scan de `sample_2`.\n",
    "\n",
    "Si on augmente la taille de `nb_val_1`, on augmente la valeur  de `n`  le nombre maximal de tuple de l'échantillon `sample_1` qui ont la même valeur sur l'attribut 1 et la complexité de `join_hash` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_1 = 100, nb_val_1=100, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0008692519986652769\n",
      "Temps pour une exécution des tris            : 0.00035252899760962464\n",
      "Temps pour une exécution de join_merge       : 0.0010969730028591584\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0006170130000100471\n",
      "Temps pour une exécution des tris            : 0.00026486000570002943\n",
      "Temps pour une exécution de join_merge       : 0.00033475999953225255\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.00044397600140655413\n",
      "Temps pour une exécution des tris            : 0.0003405249954084866\n",
      "Temps pour une exécution de join_merge       : 0.0003298110023024492\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.00038281800152617507\n",
      "Temps pour une exécution des tris            : 0.0002883930028474424\n",
      "Temps pour une exécution de join_merge       : 0.0002526640018913895\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.00033770200025173835\n",
      "Temps pour une exécution des tris            : 0.00027069000134360977\n",
      "Temps pour une exécution de join_merge       : 0.000249976001214236\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.005038141000113683\n",
      "Temps pour une exécution des tris            : 0.0016093279991764575\n",
      "Temps pour une exécution de join_merge       : 0.0064000749989645556\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0035574559988162946\n",
      "Temps pour une exécution des tris            : 0.001456437003071187\n",
      "Temps pour une exécution de join_merge       : 0.0008991319991764612\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.00279483900158084\n",
      "Temps pour une exécution des tris            : 0.0013347460007935297\n",
      "Temps pour une exécution de join_merge       : 0.00027287000193609856\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.002826602001732681\n",
      "Temps pour une exécution des tris            : 0.0014188950008247048\n",
      "Temps pour une exécution de join_merge       : 0.00024886800019885413\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.002705090999370441\n",
      "Temps pour une exécution des tris            : 0.0014358830012497492\n",
      "Temps pour une exécution de join_merge       : 0.00025422100225114264\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.047235770001861965\n",
      "Temps pour une exécution des tris            : 0.015025589000288164\n",
      "Temps pour une exécution de join_merge       : 0.06067659100153833\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.03549400100018829\n",
      "Temps pour une exécution des tris            : 0.014802580000832677\n",
      "Temps pour une exécution de join_merge       : 0.006600542001251597\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.04940681999869412\n",
      "Temps pour une exécution des tris            : 0.018181473002186976\n",
      "Temps pour une exécution de join_merge       : 0.0008758189978834707\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.031164094001724152\n",
      "Temps pour une exécution des tris            : 0.013942625002528075\n",
      "Temps pour une exécution de join_merge       : 0.00027118800062453374\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.02820695799891837\n",
      "Temps pour une exécution des tris            : 0.013815809001243906\n",
      "Temps pour une exécution de join_merge       : 0.00024297799973282963\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0003855360009765718\n",
      "Temps pour une exécution des tris            : 0.0002478220012562815\n",
      "Temps pour une exécution de join_merge       : 0.00028751100035151467\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0003836449977825396\n",
      "Temps pour une exécution des tris            : 0.00025421699683647603\n",
      "Temps pour une exécution de join_merge       : 0.0004927789996145293\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0003445670008659363\n",
      "Temps pour une exécution des tris            : 0.00026991600316250697\n",
      "Temps pour une exécution de join_merge       : 0.00027724600295186974\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.00034638200304470956\n",
      "Temps pour une exécution des tris            : 0.0002633070034789853\n",
      "Temps pour une exécution de join_merge       : 0.0002563439993537031\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0003440139989834279\n",
      "Temps pour une exécution des tris            : 0.00026091000108863227\n",
      "Temps pour une exécution de join_merge       : 0.0002730230007728096\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.003705995000927942\n",
      "Temps pour une exécution des tris            : 0.001458813996578101\n",
      "Temps pour une exécution de join_merge       : 0.003226697001082357\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0037871819986321498\n",
      "Temps pour une exécution des tris            : 0.0014722609994350933\n",
      "Temps pour une exécution de join_merge       : 0.0027184929967916105\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.003166943999531213\n",
      "Temps pour une exécution des tris            : 0.0014507949963444844\n",
      "Temps pour une exécution de join_merge       : 0.000511015001393389\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.003094130999670597\n",
      "Temps pour une exécution des tris            : 0.001582198998221429\n",
      "Temps pour une exécution de join_merge       : 0.0002727149985730648\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0027633890022116248\n",
      "Temps pour une exécution des tris            : 0.0014022129980730824\n",
      "Temps pour une exécution de join_merge       : 0.0002499530019122176\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.03333433599982527\n",
      "Temps pour une exécution des tris            : 0.017333257997961482\n",
      "Temps pour une exécution de join_merge       : 0.02762239900039276\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.03513387600105489\n",
      "Temps pour une exécution des tris            : 0.013625106996187242\n",
      "Temps pour une exécution de join_merge       : 0.02459886599899619\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.037422427001729375\n",
      "Temps pour une exécution des tris            : 0.014237604998925235\n",
      "Temps pour une exécution de join_merge       : 0.0030593269984819926\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.030589904003136326\n",
      "Temps pour une exécution des tris            : 0.014220179000403732\n",
      "Temps pour une exécution de join_merge       : 0.0004978309989382979\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.02672747099859407\n",
      "Temps pour une exécution des tris            : 0.013859255002898863\n",
      "Temps pour une exécution de join_merge       : 0.0002679190001799725\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.004471099000511458\n",
      "Temps pour une exécution des tris            : 0.0013226939991000108\n",
      "Temps pour une exécution de join_merge       : 0.005178149000130361\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.001262329999008216\n",
      "Temps pour une exécution des tris            : 0.0013743109993811231\n",
      "Temps pour une exécution de join_merge       : 0.002589702999102883\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0009553999989293516\n",
      "Temps pour une exécution des tris            : 0.0013688159997400362\n",
      "Temps pour une exécution de join_merge       : 0.0024269079985970166\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0008802470001683105\n",
      "Temps pour une exécution des tris            : 0.0013688750041183084\n",
      "Temps pour une exécution de join_merge       : 0.00237518900030409\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0008714599971426651\n",
      "Temps pour une exécution des tris            : 0.0013694770022993907\n",
      "Temps pour une exécution de join_merge       : 0.0023541640002804343\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 1000,  nb_val_2=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour une exécution de join_hash        : 0.021227352001005784\n",
      "Temps pour une exécution des tris            : 0.0024518479985999875\n",
      "Temps pour une exécution de join_merge       : 0.030134258999169106\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.006901992001075996\n",
      "Temps pour une exécution des tris            : 0.0025101810024352744\n",
      "Temps pour une exécution de join_merge       : 0.0052857890004816\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.003816141001152573\n",
      "Temps pour une exécution des tris            : 0.002549089000240201\n",
      "Temps pour une exécution de join_merge       : 0.0025737149990163743\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.00354405599864549\n",
      "Temps pour une exécution des tris            : 0.002656974997080397\n",
      "Temps pour une exécution de join_merge       : 0.0027596639993134886\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.003874024001561338\n",
      "Temps pour une exécution des tris            : 0.002621509000164224\n",
      "Temps pour une exécution de join_merge       : 0.002578445000835927\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.25676219099841546\n",
      "Temps pour une exécution des tris            : 0.014961007000238169\n",
      "Temps pour une exécution de join_merge       : 0.32369326399930287\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.05505885899765417\n",
      "Temps pour une exécution des tris            : 0.01805923400024767\n",
      "Temps pour une exécution de join_merge       : 0.029088708000927\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.03826846199808642\n",
      "Temps pour une exécution des tris            : 0.01694315100030508\n",
      "Temps pour une exécution de join_merge       : 0.004965110001649009\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.02905406299760216\n",
      "Temps pour une exécution des tris            : 0.016152421001606854\n",
      "Temps pour une exécution de join_merge       : 0.0028371089974825736\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.03745083600006183\n",
      "Temps pour une exécution des tris            : 0.020288192998123122\n",
      "Temps pour une exécution de join_merge       : 0.002466083999024704\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.001326993999100523\n",
      "Temps pour une exécution des tris            : 0.0014242609977372922\n",
      "Temps pour une exécution de join_merge       : 0.000804121998953633\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0017117399984272197\n",
      "Temps pour une exécution des tris            : 0.0015087039973877836\n",
      "Temps pour une exécution de join_merge       : 0.0030187919983291067\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0009760500033735298\n",
      "Temps pour une exécution des tris            : 0.0014851119995000772\n",
      "Temps pour une exécution de join_merge       : 0.0027945679976255633\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0012458720011636615\n",
      "Temps pour une exécution des tris            : 0.001723588997265324\n",
      "Temps pour une exécution de join_merge       : 0.0027770529995905235\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0009208609990309924\n",
      "Temps pour une exécution des tris            : 0.0014765919986530207\n",
      "Temps pour une exécution de join_merge       : 0.0024837640012265183\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.005514853997738101\n",
      "Temps pour une exécution des tris            : 0.0026494640042074025\n",
      "Temps pour une exécution de join_merge       : 0.005636835001496365\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.009604668000974925\n",
      "Temps pour une exécution des tris            : 0.0030564349981432315\n",
      "Temps pour une exécution de join_merge       : 0.008563358001993038\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.004139933000260498\n",
      "Temps pour une exécution des tris            : 0.002671217003808124\n",
      "Temps pour une exécution de join_merge       : 0.003389051998965442\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0036024959990754724\n",
      "Temps pour une exécution des tris            : 0.0026323210004193243\n",
      "Temps pour une exécution de join_merge       : 0.0030416439985856414\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.005158482999831904\n",
      "Temps pour une exécution des tris            : 0.0035203830011596438\n",
      "Temps pour une exécution de join_merge       : 0.003556106999894837\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.04966550800236291\n",
      "Temps pour une exécution des tris            : 0.01457755800220184\n",
      "Temps pour une exécution de join_merge       : 0.06108133599991561\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.051823231999151176\n",
      "Temps pour une exécution des tris            : 0.01480262899713125\n",
      "Temps pour une exécution de join_merge       : 0.059406914999271976\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.03766508300032001\n",
      "Temps pour une exécution des tris            : 0.015619771998899523\n",
      "Temps pour une exécution de join_merge       : 0.007640127001650399\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.030869644997437717\n",
      "Temps pour une exécution des tris            : 0.018084499999531545\n",
      "Temps pour une exécution de join_merge       : 0.0036453749999054708\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.03547072199944523\n",
      "Temps pour une exécution des tris            : 0.016467813999042846\n",
      "Temps pour une exécution de join_merge       : 0.0026241070008836687\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.03934209200087935\n",
      "Temps pour une exécution des tris            : 0.013612136001029285\n",
      "Temps pour une exécution de join_merge       : 0.0513756000000285\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.01051051499962341\n",
      "Temps pour une exécution des tris            : 0.013998728998558363\n",
      "Temps pour une exécution de join_merge       : 0.030661618999147322\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0075463350003701635\n",
      "Temps pour une exécution des tris            : 0.012886128995887702\n",
      "Temps pour une exécution de join_merge       : 0.025605053000617772\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.006881022000015946\n",
      "Temps pour une exécution des tris            : 0.01273162000143202\n",
      "Temps pour une exécution de join_merge       : 0.02405947200168157\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.007373836997430772\n",
      "Temps pour une exécution des tris            : 0.01319308500023908\n",
      "Temps pour une exécution de join_merge       : 0.02445060399986687\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 10000,  nb_val_2=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour une exécution de join_hash        : 0.2386652020031761\n",
      "Temps pour une exécution des tris            : 0.01600561099985498\n",
      "Temps pour une exécution de join_merge       : 0.3190481269994052\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.03810174200043548\n",
      "Temps pour une exécution des tris            : 0.013746472999628168\n",
      "Temps pour une exécution de join_merge       : 0.046791820001089945\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.011353731999406591\n",
      "Temps pour une exécution des tris            : 0.013935019000200555\n",
      "Temps pour une exécution de join_merge       : 0.026105267999810167\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.009515468998870347\n",
      "Temps pour une exécution des tris            : 0.014380983997398289\n",
      "Temps pour une exécution de join_merge       : 0.02463386899762554\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.011584324998693774\n",
      "Temps pour une exécution des tris            : 0.015221690002363175\n",
      "Temps pour une exécution de join_merge       : 0.024129669000103604\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 2.0991156039999623\n",
      "Temps pour une exécution des tris            : 0.025313342001027195\n",
      "Temps pour une exécution de join_merge       : 2.8111052719978034\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.27229684200210613\n",
      "Temps pour une exécution des tris            : 0.026536533998296363\n",
      "Temps pour une exécution de join_merge       : 0.30131399300080375\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.07444535499962512\n",
      "Temps pour une exécution des tris            : 0.0271147609964828\n",
      "Temps pour une exécution de join_merge       : 0.0502741939999396\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.04063328600022942\n",
      "Temps pour une exécution des tris            : 0.027644443995086476\n",
      "Temps pour une exécution de join_merge       : 0.027526993999344995\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.034414673998981016\n",
      "Temps pour une exécution des tris            : 0.02602276000470738\n",
      "Temps pour une exécution de join_merge       : 0.024616974998934893\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.009905822000291664\n",
      "Temps pour une exécution des tris            : 0.013622890000988264\n",
      "Temps pour une exécution de join_merge       : 0.005252470000414178\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.010435529002279509\n",
      "Temps pour une exécution des tris            : 0.014164503001666162\n",
      "Temps pour une exécution de join_merge       : 0.027470414999697823\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.007742707999568665\n",
      "Temps pour une exécution des tris            : 0.0138196810039517\n",
      "Temps pour une exécution de join_merge       : 0.025850187998003094\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.006344625999190612\n",
      "Temps pour une exécution des tris            : 0.013672097000380745\n",
      "Temps pour une exécution de join_merge       : 0.024767554001300596\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.006371254999976372\n",
      "Temps pour une exécution des tris            : 0.013664756999787642\n",
      "Temps pour une exécution de join_merge       : 0.024119373996654758\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.027690081999026006\n",
      "Temps pour une exécution des tris            : 0.014605018997826846\n",
      "Temps pour une exécution de join_merge       : 0.03128351899795234\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.04288936499870033\n",
      "Temps pour une exécution des tris            : 0.01468099900012021\n",
      "Temps pour une exécution de join_merge       : 0.05186300599962124\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.01347374699980719\n",
      "Temps pour une exécution des tris            : 0.014925292998668738\n",
      "Temps pour une exécution de join_merge       : 0.028192484998726286\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.009425081003428204\n",
      "Temps pour une exécution des tris            : 0.014599441001337254\n",
      "Temps pour une exécution de join_merge       : 0.02762897900174721\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.009237135000148555\n",
      "Temps pour une exécution des tris            : 0.014726135996170342\n",
      "Temps pour une exécution de join_merge       : 0.024103203999402467\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.2329853709998133\n",
      "Temps pour une exécution des tris            : 0.026114259999303613\n",
      "Temps pour une exécution de join_merge       : 0.30470474300091155\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.3033694789992296\n",
      "Temps pour une exécution des tris            : 0.0268575790032628\n",
      "Temps pour une exécution de join_merge       : 0.3598903199999768\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.08224774100017385\n",
      "Temps pour une exécution des tris            : 0.02789378499801387\n",
      "Temps pour une exécution de join_merge       : 0.05340585199883208\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.04243945000052918\n",
      "Temps pour une exécution des tris            : 0.02999596500012558\n",
      "Temps pour une exécution de join_merge       : 0.02829688799829455\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.03549547100192285\n",
      "Temps pour une exécution des tris            : 0.027148797998961527\n",
      "Temps pour une exécution de join_merge       : 0.02461250299893436\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for size_2 in [10**k for k in range(2, 5)]:    \n",
    "    for nb_val_2 in [10**k for k in range(2, 4)]:\n",
    "        for size_1 in [10**k for k in range(2, 5)]: \n",
    "            for nb_val_1 in [10**k for k in range(2, 7)]:\n",
    "                print(f\"size_1 = {size_1}, nb_val_1={nb_val_1}, size_2 = {size_2},  nb_val_2={nb_val_2}\")\n",
    "                benchmark(bench_loop=False, size_1 = size_1, nb_val_1 =nb_val_1  ,size_2 = size_2, nb_val_2 = nb_val_2, nb_repeat = 10)\n",
    "                print('-'* 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que `join_merge` est beaucoup plus rapide lorsque `size_1` est beaucoup plus grand que `size_2` et d'autant plus que `nb_val_1` est grand (et donc la longueur maximale d'une liste stockée dans l'index/table de hash)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE (POUR ALLER PLUS LOIN)** : même question que précédemment, mais cette fois si il faut trouver un cas qui est favorable à `join_nested_loop` et dévaforable aux deux autres. _Indice_ faites en sorte que la jointure soit aussi grosse que le produit cartésien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour une exécution de join_nested_loop : 0.1361710969977139\n",
      "Temps pour une exécution de join_hash        : 0.13963669299846515\n",
      "Temps pour une exécution des tris            : 0.0021062710002297536\n",
      "Temps pour une exécution de join_merge       : 0.23856367899861652\n"
     ]
    }
   ],
   "source": [
    "#il suffit de prende la même constante pour les attributs de jointure\n",
    "#en choisissant aléatoirement sa valeur dans un ensemble de taille 1\n",
    "\n",
    "benchmark(size_1=100, nb_val_1=1,\n",
    "              size_2=100, nb_val_2=1,\n",
    "              nb_repeat=100,\n",
    "              bench_loop=True, bench_hash=True, bench_merge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparer l'exécution dans Python à celle native dans SQLite\n",
    "-------------------------------------------------------------\n",
    "\n",
    "Maintenant, on va comparer la performance de ces implantations Python face aux algorithmes jointures de SQLite (qui est écrit en C). Pour cela on va comparer les deux approches suivantes :\n",
    "\n",
    "* **Approche A : jointure en SQLite**, on exécute la requête `SELECT * FROM table1 JOIN table2 ON table1.val == table2.val` puis (depuis Python) on récupère l'intégralité du résultat, c'est la fonction `join_python()`\n",
    "* **Approche B : jointure en Python**, on exécute la requête `SELECT * FROM table1` et on stocke son résultat dans un tableau, de même pour `SELECT * FROM table2` puis on utilise un des algorithmes précedents pour faire le calcul de jointure et enfin on renvoie le résultat, c'est la fonction `join_sqlite()`\n",
    "\n",
    "\n",
    "**EXERCICE** : créer une nouvelle base de données nommée `join_algorithms_versus_sqlite3.db` et exécuter le script SQL `join_algorithms_schema.sql` pour créer le schéma *et* peupler la base avec un jeu de données similaire à celui du benchmark de l'exercice précédent.\n",
    "\n",
    "**EXERCICE** : en vous inspirant du code fourni, compléter les fonctions `join_python()` et `join_sqlite()` du programme [`join_algorithms_versus_sqlite3.py`](join_algorithms_versus_sqlite3.py) et observer les temps d'exécution.\n",
    "\n",
    " **EXERCICE** : ensuite, exécuter la requête de jointure directement dans `SQLite3` en activant activant le chronométrage avec `.timer on` depuis l'interpréteur ligne de commande ou depuis _DB Browser for SQLite_ (le temps est affiché en bas de la fenêtre d'exécution). Vous devriez avoir une différence _de plusieurs ordre de magnitude entre les deux_ : comment l'expliquer ?\n",
    "\n",
    " **EXERCICE** : reprendre la comparaison mais cette fois avec la requête `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val`. Ici, `join_python()` renverra _la longueur du tableau_, comme par exemple `len(join_hash(table1, 1, table2, 0))` pour l'algorithme de jointure par hash. Comparer les temps d'exécution, une différence _de plusieurs ordre de magnitude doit les séparer_  : comment l'expliquer ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.displaycon = False\n",
    "%config SqlMagic.autolimit = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql sqlite:///join_algorithms_versus_sqlite3.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "PRAGMA foreign_keys=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "-- POUR CHARGER DANS SQLite3 : \n",
    "-- sqlite3 join_algorithms_versus_sqlite3.db\n",
    "-- .timer on\n",
    "-- .read join_algorithms_schema.sql\n",
    "\n",
    "drop table if exists Table1;\n",
    "drop table if exists Table2;\n",
    "\n",
    "-- Un schéma très simple, qui reprend le benchmark des 3 algos en Python qu'on rappelle ici :\n",
    "--\n",
    "-- def benchmark(size_1=1000, nb_val_1=100,\n",
    "--               size_2=1000, nb_val_2=100,\n",
    "--               nb_repeat=100,\n",
    "--               bench_loop=True, bench_hash=True, bench_merge=True):\n",
    "--\n",
    "--    sample_1 = [(i, randrange(nb_val_1)) for i in range(size_1)]\n",
    "--    sample_2 = [(randrange(nb_val_2), 'A'+str(j)) for j in range(size_2)]\n",
    "\n",
    "-- Activation des clefs étrangères\n",
    "PRAGMA foreign_keys=1; \n",
    "\n",
    "create table Table1(\n",
    "    idA INTEGER PRIMARY KEY,    -- un ID entier\n",
    "    val INTEGER     -- l'attribut de jointure\n",
    ");\n",
    "\n",
    "create table Table2(\n",
    "    val INTEGER,  -- pas  REFERENCES Table1(val), mais ca ne changerait rien\n",
    "    idB INTEGER\n",
    ");\n",
    "\n",
    "\n",
    " -- On va remplir les tables avec 10.000 lignes pour A et autant pour B\n",
    "\n",
    "--pour générer récursivement tous les entiers entre 1 et 10000\n",
    "--voir https://sqlpro.developpez.com/cours/sqlserver/cte-recursives/\n",
    "WITH RECURSIVE data_1(val) AS (\n",
    "  SELECT 1\n",
    "  UNION ALL\n",
    "  SELECT val+1\n",
    "  FROM data_1\n",
    "  WHERE val+1 <= 10000\n",
    ") \n",
    "INSERT INTO Table1 \n",
    "  SELECT  val,\n",
    "          ABS(RANDOM() % 1000)   --entiers aléatoires entre 0 et 1000\n",
    "  FROM data_1;\n",
    "\n",
    "WITH RECURSIVE data_2(val) AS (\n",
    "  SELECT 1\n",
    "  UNION ALL\n",
    "  SELECT val+1\n",
    "  FROM data_2\n",
    "  WHERE val+1 <= 10000\n",
    ") \n",
    "INSERT INTO Table2\n",
    "  SELECT  ABS(RANDOM() % 1000),\n",
    "          'A' || val\n",
    "  FROM data_2;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Un module pour comparer la performance des jointures 'à la main'\n",
    "    en Python face à ceux en C bien choisis de SQLite3\"\"\"\n",
    "\n",
    "import sqlite3\n",
    "import logging\n",
    "#voir https://docs.python.org/3/howto/logging.html\n",
    "from timeit import timeit\n",
    "#from join_algorithms import join_nested_loop, join_hash, join_merge\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "DB_FILE = 'join_algorithms_versus_sqlite3.db'\n",
    "\n",
    "def join_algorithms_versus_sqlite3():\n",
    "    \"\"\"Fonction de comparaison\"\"\"\n",
    "    try:\n",
    "        connection = sqlite3.connect(DB_FILE)\n",
    "\n",
    "        # Pour avoir des ditcionnaires et non des tuples dans le résultat\n",
    "        # see https://docs.python.org/3.6/library/sqlite3.html#sqlite3.Row\n",
    "        # connection.row_factory = sqlite3.Row\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        logging.debug(\"You are connected to - %s\", DB_FILE)\n",
    "        cursor.execute(\"SELECT sqlite_version() as version;\")\n",
    "        record = cursor.fetchone()\n",
    "        logging.debug(tuple(record))\n",
    "\n",
    "        def join_python(jointure):\n",
    "            cursor.execute(\"SELECT * FROM table1;\")\n",
    "            table1 = cursor.fetchall()\n",
    "            #print(table1[:10])\n",
    "            cursor.execute(\"SELECT * FROM table2;\")\n",
    "            table2 = cursor.fetchall()\n",
    "            jointure(table1, 1,table2, 0)\n",
    "            \n",
    "        def join_sqlite():\n",
    "            cursor.execute(\"SELECT * FROM table1 JOIN table2 ON table1.val == table2.val;\")\n",
    "            record = cursor.fetchone()\n",
    "            \n",
    "        for jointure in [join_hash, join_merge]:\n",
    "            time_join_python = timeit(lambda : join_python(jointure), number=100)\n",
    "            logging.info(f'Temps pour une jointure {jointure.__name__} côté Python : %f', time_join_python)\n",
    "\n",
    "        time_join_sqlite = timeit(join_sqlite, number=100)\n",
    "        logging.info('Temps pour une jointure côté Sqlite3 : %f', time_join_sqlite)\n",
    "\n",
    "    except (sqlite3.Error) as error:\n",
    "        logging.error(\"Error while connecting to sqlite3: %s\", error)\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            logging.debug(\"Sqlite3 connection is closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Temps pour une jointure join_hash côté Python : 4.491942\n",
      "INFO:root:Temps pour une jointure join_merge côté Python : 1.616048\n",
      "INFO:root:Temps pour une jointure côté Sqlite3 : 0.878571\n"
     ]
    }
   ],
   "source": [
    "join_algorithms_versus_sqlite3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **EXERCICE** : ensuite, exécuter la requête de jointure directement dans `SQLite3` en activant activant le chronométrage avec `.timer on` depuis l'interpréteur ligne de commande ou depuis _DB Browser for SQLite_ (le temps est affiché en bas de la fenêtre d'exécution). Vous devriez avoir une différence _de plusieurs ordre de magnitude entre les deux_ : comment l'expliquer ?\n",
    " \n",
    "* sqlite3 CLI :\n",
    "\n",
    "![sqlite3-CLI](time_sqlite3.png)\n",
    "\n",
    "* sqlite DBBrowser :\n",
    "\n",
    "![sqlite DBBrowser](time_dbbrowser.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE** : reprendre la comparaison mais cette fois avec la requête `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val`. Ici, `join_python()` renverra _la longueur du tableau_, comme par exemple `len(join_hash(table1, 1, table2, 0))` pour l'algorithme de jointure par hash. Comparer les temps d'exécution, une différence _de plusieurs ordre de magnitude doit les séparer_  : comment l'expliquer ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_algorithms_versus_sqlite3_V2():\n",
    "    \"\"\"Fonction de comparaison\"\"\"\n",
    "    try:\n",
    "        connection = sqlite3.connect(DB_FILE)\n",
    "\n",
    "        # Pour avoir des ditcionnaires et non des tuples dans le résultat\n",
    "        # see https://docs.python.org/3.6/library/sqlite3.html#sqlite3.Row\n",
    "        # connection.row_factory = sqlite3.Row\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        logging.debug(\"You are connected to - %s\", DB_FILE)\n",
    "        cursor.execute(\"SELECT sqlite_version() as version;\")\n",
    "        record = cursor.fetchone()\n",
    "        logging.debug(tuple(record))\n",
    "\n",
    "        def join_python(jointure):\n",
    "            cursor.execute(\"SELECT * FROM table1;\")\n",
    "            table1 = cursor.fetchall()\n",
    "            #print(table1[:10])\n",
    "            cursor.execute(\"SELECT * FROM table2;\")\n",
    "            table2 = cursor.fetchall()           \n",
    "            return len( jointure(table1, 1,table2, 0))\n",
    "            \n",
    "        def join_sqlite():\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val;\")\n",
    "            record = cursor.fetchone()\n",
    "            return record\n",
    "        \n",
    "                \n",
    "        for jointure in [join_hash, join_merge]:\n",
    "            time_join_python = timeit(lambda : join_python(jointure), number=100)\n",
    "            logging.info(f'Temps pour une jointure {jointure.__name__} côté Python : %f', time_join_python)\n",
    "            \n",
    "        time_join_sqlite = timeit(join_sqlite, number=100)\n",
    "        logging.info('Temps pour une jointure côté Sqlite3 : %f', time_join_sqlite)\n",
    "\n",
    "        \n",
    "\n",
    "    except (sqlite3.Error) as error:\n",
    "        logging.error(\"Error while connecting to sqlite3: %s\", error)\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            logging.debug(\"Sqlite3 connection is closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Temps pour une jointure join_hash côté Python : 4.571115\n",
      "INFO:root:Temps pour une jointure join_merge côté Python : 1.639042\n",
      "INFO:root:Temps pour une jointure côté Sqlite3 : 1.563244\n"
     ]
    }
   ],
   "source": [
    "join_algorithms_versus_sqlite3_V2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas, on a un facteur 100 entre l'exécution en ligne de commandes et l'exécution à travers un programme Python. La requête `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val;` est plus lente que  `SELECT * FROM table1 JOIN table2 ON table1.val == table2.val;` depuis un programme Python si on l'exécute dans `sqlite3` d'abord et similaire si on fait la jointure après avec nos fonctions maisons. On observe au contraire que la  requête  `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val;`  s'exécute plus rapidement en ligne de commande.\n",
    "\n",
    "\n",
    "* sqlite3 CLI :\n",
    "\n",
    "![sqlite3-CLI](timer2_sqlite3.png)\n",
    "\n",
    "* sqlite DBBrowser :\n",
    "\n",
    "![sqlite DBBrowser](timer2_sqliteDBBrowser.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EXERCICE (FACULTATIF ET OUVERT) :__\n",
    "\n",
    "Conclure en formulant quelques bonnes pratiques de l'accès à une base de données via un programme (Python).\n",
    "\n",
    "\n",
    "* Bonne pratique 1 : Faire les opérations SQL (jointures) dans SQLite et non dans Python\n",
    "* Bonne pratique 2 : se méfier des tests en ligne de commandes qui peuvent donner de mauvaises indications de performances (voir exercice précédent avec les requêtes `SELECT * FROM table1 JOIN table2 ON table1.val == table2.val;` et `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val;`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
