{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIU bloc 4 : \"Bases de données : création de schémas et normalisation\" : TP sur les algorithmes de jointure\n",
    "====================================================\n",
    "\n",
    "Dans ce TP, on va s'intéresser **aux algorithmes de jointures**, c'est-à-dire aux algorithmes exécutés par les moteurs des SGBDs quand ils traduisent des requêtes comme la suivante :\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM table1 JOIN table2 ON table1.attr1 == table2.attr2\n",
    "```\n",
    "\n",
    "Il existe plusieurs algorithmes de jointure et l'optimiseur de requêtes du SGBD va tâcher de choisir le _meilleur_, vis-à-vis de statistiques sur les données et surtout des **index** disponibles sur les tables.\n",
    "Le but du TP est ainsi de comprendre ces algorithmes fondamentaux et de les comparer entre eux puis de les comparer face à deux de SQLite 3.\n",
    "\n",
    "**Remarque** la comparaison de performance (_benchmark_) est un exercice complexe car de nombreux paramètres très différents contribuent à la performance finale (matériel, OS, I/O disques ou d'affichage, efficacité de la compilation/interprétation du langage de programmation, caches, temps d'initialisation etc.).\n",
    "\n",
    "Implanter les algorithmes classiques de jointure en Python\n",
    "-----------------------------------------------------------\n",
    "\n",
    "Le fichier [`join_algorithms.py`](join_algorithms.py) contient le squelette à remplir pour les trois algorithmes, à savoir _nested loop_, _hash join_ et _merge join_. Ces algorithmes font la même chose et ont la même signature `def algo(table1, attr1, table2, attr2):` :\n",
    "\n",
    "* `table1` et `table2` sont des listes (Python) de tuples (Python). Il n'y a pas de garanties d'ordre sur ces listes;\n",
    "* `attr1` (resp. `attr2`) est _l'indice_ (entier) de l'attribut de `table1` (resp. de `tablee`) sur lequel on fait la jointure;\n",
    "* ces algorithmes retournent tous une liste de tuples, comme l'aurait fait la requête SQL.\n",
    "\n",
    "\n",
    "Le fichier [`join_algorithms_test.py`](join_algorithms_test.py) donne un exemple d'entrées et de résultats attendus.\n",
    "\n",
    "**EXERCICE** : compléter la fonction `join_nested_loop` et tester votre implantation avec `pytest-3` et les tests fournis.\n",
    "\n",
    "**EXERCICE** : compléter la fonction `join_hash` et tester votre implantation avec `pytest-3` et les tests fournis.\n",
    "\n",
    "**EXERCICE (POUR ALLER PLUS LOIN)** : compléter la fonction `join_merge` et tester votre implantations avec `pytest-3` et les tests fournis. Vous n'êtes pas obligé de faire cet exercice pour passer à la suite.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Un module pour illustrer les principaux alogorithmes de jointure en Python\n",
    "    On ne gèrera ici le cas nominal d'utilisation des fonctions où\n",
    "    attr1 et attr2 sont LES INDEX EXISTANT des attributs\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from random import randrange\n",
    "from timeit import timeit\n",
    "from collections import defaultdict\n",
    "# voir https://docs.python.org/3.6/library/collections.html#collections.defaultdict\n",
    "# est utile pour le hash join en initialisant à la liste vide\n",
    "\n",
    "\n",
    "\n",
    "def join_nested_loop(table1, attr1, table2, attr2):\n",
    "    \"\"\"L'algorithme naif de jointure par deux boucles imbriquées :\n",
    "       pour chaque tuple de tabl1, on va lire toute la table2\n",
    "       et produire un résultat à chaque fois que tup1[attr1] == tup2[attr2]\n",
    "       https://en.wikipedia.org/wiki/Nested_loop_join\n",
    "\n",
    "       CONTRAT: pas de préconditions sur les entrées\"\"\"\n",
    "    return [t1 + t2 for t1 in table1 for t2 in table2 if t1[attr1] == t2[attr2]]\n",
    "\n",
    "\n",
    "def join_hash(table1, attr1, table2, attr2):\n",
    "    \"\"\"L'algorithme de jointure en deux étapes :\n",
    "          1. construction d'un index sur table1 via un dictionnaire :\n",
    "             à chaque valeur de attr1, on associe **la liste** des index des\n",
    "             tuples de table1 qui ont cette valeur\n",
    "          2. scan de table2 :\n",
    "             pour chaque tuple, on va chercher AVEC L'INDEX la liste\n",
    "             des tuples de table1 qui on la valeur de attr2 pour attr1\n",
    "             On fait attention aux collisions de hash et on ajoute les\n",
    "             tuples au résultat.\n",
    "\n",
    "        https://en.wikipedia.org/wiki/Hash_join#Classic_hash_join\n",
    "\n",
    "        CONTRAT: pas de préconditions sur les entrées\"\"\"\n",
    "    res = []\n",
    "    index = dict()\n",
    "    for k, t1 in enumerate(table1):\n",
    "        val = t1[attr1]\n",
    "        #pour gérer les collisions de hash\n",
    "        #on stocke dans index[val] une liste de listes dont le premier élément est t1[attr1]\n",
    "        #et dont les éléments suivants sont les index des tuples de table1 qui ont même\n",
    "        #même valeur val pour l'attribut d'index attr1\n",
    "        if val in index:\n",
    "            for l in  index[val]:\n",
    "                if l[0] == val:\n",
    "                    l.append(k)\n",
    "                    break\n",
    "        else:\n",
    "            index[val] = [[val, k]]\n",
    "    for t2 in table2:\n",
    "        val = t2[attr2]\n",
    "        if val in index:\n",
    "            for l in index[val]:\n",
    "                if l[0] == val:\n",
    "                    for k in l[1:]:\n",
    "                        res.append(table1[k] + t2)\n",
    "    return res\n",
    "\n",
    "def join_hash2(table1, attr1, table2, attr2):\n",
    "    \"\"\"L'algorithme de jointure en deux étapes :\n",
    "          1. construction d'un index sur table1 via un dictionnaire :\n",
    "             à chaque valeur de attr1, on associe **la liste** des index des\n",
    "             tuples de table1 qui ont cette valeur\n",
    "          2. scan de table2 :\n",
    "             pour chaque tuple, on va chercher AVEC L'INDEX la liste\n",
    "             des tuples de table1 qui on la valeur de attr2 pour attr1\n",
    "             On fait attention aux collisions de hash et on ajoute les\n",
    "             tuples au résultat.\n",
    "\n",
    "        https://en.wikipedia.org/wiki/Hash_join#Classic_hash_join\n",
    "\n",
    "        CONTRAT: pas de préconditions sur les entrées\"\"\"\n",
    "    # en fait, comme les dictionnaires Python utilisent des tables de hash\n",
    "    # il suffit d'un dictionnaire qui sert d'index (un peu comme pour le\n",
    "    # décorateur memoize) pour implémenter cet algorithme\n",
    "    #hash_table = defaultdict(list)\n",
    "    #pass\n",
    "    res = []\n",
    "    index =defaultdict(list)\n",
    "    for k, t1 in enumerate(table1):\n",
    "        val = t1[attr1]\n",
    "        #pour gérer les collisions de hash\n",
    "        #on stocke dans index[val] une liste de listes dont le premier élément est t1[attr1]\n",
    "        #et dont les éléments suivants sont les index des tuples de table1 qui ont même\n",
    "        #même valeur val pour l'attribut d'index attr1\n",
    "        index[val].append(k)\n",
    "    for t2 in table2:\n",
    "        val = t2[attr2]\n",
    "        if val in index:\n",
    "            for k in index[val]:\n",
    "                res.append(table1[k] + t2)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def join_merge(table1, attr1, table2, attr2):\n",
    "    \"\"\"L'algorithme de jointure sort-merge qui s'appuie sur des tables SUPPOSEES TRIEES\n",
    "       https://en.wikipedia.org/wiki/Sort-merge_join\n",
    "       Son principe est assez similaire à l'étape \"merge\" du merge sort\n",
    "       https://en.wikipedia.org/wiki/Merge_sort\n",
    "\n",
    "       On avance en // sur table1 et table2 avec 2 index ind1 et ind2\n",
    "        - si table1[ind1][attr1] est avant table2[ind2][attr2]\n",
    "          on incrémente ind1\n",
    "        - si table2[ind2][attr2] est avant table1[ind1][attr1]\n",
    "          on incrémente ind2\n",
    "        - si les tuples sur lesquels on se trouve respectent la condition\n",
    "                table1[ind1][attr1] == table2[ind2][attr2]\n",
    "           alors avec une boucle locale, on va chercher tous les tuples\n",
    "           de table2 satisfont la condition et ajouter au résultat.\n",
    "           ensuite on incrémente ind1\n",
    "\n",
    "       CONTRAT : le trie des entrées est à la charge des utilisateurs,\n",
    "                 le comportement n'est pas garanti sinon\"\"\"\n",
    "\n",
    "    res = []\n",
    "    n1 = len(table1)\n",
    "    n2 = len(table2)\n",
    "    ind1 = ind2 = 0\n",
    "    while ind1 < n1 and ind2 < n2:\n",
    "        t1, t2 = table1[ind1], table2[ind2]\n",
    "        if t1[attr1] < t2[attr2]:\n",
    "            ind1 += 1\n",
    "        elif t1[attr1] > t2[attr2]:\n",
    "            ind2 += 1\n",
    "        else:\n",
    "            ind3 = ind2\n",
    "            while ind3 < n2 and table2[ind3][attr2] == t1[attr1]:\n",
    "                res.append(t1 + table2[ind3])\n",
    "                ind3 += 1\n",
    "            ind1 += 1\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "fjunier@fjunier:~/Git/DIU-Junier/bloc4/BDD_Conception_Normalisation/sandbox$ pytest-3 -v\n",
    "============================= test session starts ==============================\n",
    "platform linux -- Python 3.6.9, pytest-3.3.2, py-1.5.2, pluggy-0.6.0 -- /usr/bin/python3\n",
    "cachedir: .cache\n",
    "rootdir: /home/fjunier/Git/DIU-Junier/bloc4/BDD_Conception_Normalisation/sandbox, inifile:\n",
    "plugins: Faker-4.1.0\n",
    "collected 3 items\n",
    "\n",
    "join_algorithms_test.py::test_join_nested_loop PASSED                    [ 33%]\n",
    "join_algorithms_test.py::test_join_hash PASSED                           [ 66%]\n",
    "join_algorithms_test.py::test_join_merge PASSED                          [100%]\n",
    "\n",
    "=========================== 3 passed in 0.07 seconds ===========================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Comparer algorithmes implantés en Python\n",
    "----------------------------------------\n",
    "\n",
    "On peut maintenant comparer la performance des algorithmes avec la fonction fournie `benchmark`.Pour la fonction `join_merge` on compte séparément le temps pris pour le tris des tables.\n",
    "En effet, cette étape peut-être _amortie_ car elle est utile pour d'autre opérations que la jointure, comme les clauses `ORDER BY` ou `GROUP BY`.\n",
    "\n",
    "**EXERCICE** : comprendre ce que fait la fonction `benchmark` (vous pouvez ajouter des commentaires par exempl)e avant de l'exécuter. \n",
    "\n",
    "Avec les paramètres par défaut de `benchmark`, on obtient les résultats suivants sur une machine portable (Dual Core Intel i7-5600U CPU @ 2.60GHz, 8GB RAM).\n",
    "\n",
    "```\n",
    "Temps pour une exécution de join_nested_loop : 4.8441446340002585\n",
    "Temps pour une exécution de join_hash        : 0.1884105869976338\n",
    "Temps pour une exécution des tris            : 0.018489982991013676\n",
    "Temps pour une exécution de join_merge       : 0.3174076739960583\n",
    "```\n",
    "\n",
    "**EXERCICE (POUR ALLER PLUS LOIN)** : jouer avec les paramètres pour trouver un cas qui soit défavorable à `join_hash` mais favorable à `join_merge`. Sans tenir compte du temps de tri, on peut trouver des cas avec un facteur 10x en faveur de `join_merge`. _Indice_ : remarquez que les rôles de `table1` et `table2` sont asymétriques faire en sorte de passer du temps dans l'étape de construction d'index de `join_hash`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(size_1=1000, nb_val_1=100,\n",
    "              size_2=1000, nb_val_2=100,\n",
    "              nb_repeat=100,\n",
    "              bench_loop=True, bench_hash=True, bench_merge=True):\n",
    "    \"\"\"Compare les différentes implémentations\"\"\"\n",
    "\n",
    "\n",
    "    sample_1 = [(i, randrange(nb_val_1)) for i in range(size_1)]\n",
    "    sample_2 = [(randrange(nb_val_2), 'A'+str(j)) for j in range(size_2)]\n",
    "\n",
    "    if bench_loop:\n",
    "        time_loop = timeit(lambda: join_nested_loop(sample_1, 1, sample_2, 0), number=nb_repeat)\n",
    "        print(f'Temps pour une exécution de join_nested_loop : {1000*time_loop/nb_repeat} ms')\n",
    "\n",
    "    if bench_hash:\n",
    "        time_hash = timeit(lambda: join_hash(sample_1, 1, sample_2, 0), number=nb_repeat)\n",
    "        print(f'Temps pour une exécution de join_hash        : {1000*time_hash/nb_repeat} ms')\n",
    "\n",
    "    if bench_merge:\n",
    "        time_sort = timeit(lambda: sample_1.sort(key=lambda x: x[1]), number=nb_repeat)\n",
    "        time_sort += timeit(lambda: sample_2.sort(key=lambda x: x[0]), number=nb_repeat)\n",
    "        time_merge = timeit(lambda: join_merge(sample_1, 1, sample_2, 0), number=nb_repeat)\n",
    "        print(f'Temps pour une exécution des tris            : {1000*time_sort/nb_repeat} ms')\n",
    "        print(f'Temps pour une exécution de join_merge       : {1000*time_merge/nb_repeat} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour une exécution de join_nested_loop : 65.0054359400383 ms\n",
      "Temps pour une exécution de join_hash        : 2.1984532799979206 ms\n",
      "Temps pour une exécution des tris            : 0.22089238991611637 ms\n",
      "Temps pour une exécution de join_merge       : 3.245223170015379 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Test avec join_hash première version\n",
    "In [2]: benchmark()\n",
    "Temps pour une exécution de join_nested_loop : 6.241203233000078\n",
    "Temps pour une exécution de join_hash        : 0.16518595900015498\n",
    "Temps pour une exécution des tris            : 0.020488148000140427\n",
    "Temps pour une exécution de join_merge       : 0.3143313499999749\n",
    "```\n",
    "\n",
    "```\n",
    "Test avec join_hash seconde version\n",
    "In [4]: benchmark()\n",
    "Temps pour une exécution de join_nested_loop : 6.2457434129999\n",
    "Temps pour une exécution de join_hash        : 0.162923426000134\n",
    "Temps pour une exécution des tris            : 0.02036094400023103\n",
    "Temps pour une exécution de join_merge       : 0.2916302970002107\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de complexité \n",
    "\n",
    "Soit `m` le nombre de lignes dans la jointure. Ce nombre est majoré par `max(nb_val_1, nb_val_2)`\n",
    "\n",
    "* Complexité de `join_nested_loop` : `O(size_1 * size_2)`\n",
    "* Complexité de `join_hash` :  `O(size_1 * n + size_2 * n)` (majoration grossière) où `n` est le nombre maximal de tuple de l'échantillon `sample_1` qui ont la même valeur sur l'attribut 1 (longueur maximale d'une liste d'indexs stockée dans l'index/table de hash) : la valeur de `n` est d'autant plus grande que le domaine de `sample_1[1]` de taille `nb_val_1` est grand. On peut donc prendre comme majorant `O(size_1 * nb_val_1 + size_2 * nb_val_1)` \n",
    "* Complexité de `join_merge` (sans compter le tri des tables) :  `O(size_1 + size_2 + size_2 * m)` où `m` est le nombre de lignes dans la jointure.\n",
    "\n",
    "\n",
    "Si `size_1 == size_2` et `nb_val_1 == nb_val_2`, comme les valeurs sont choisies selon une loi uniforme dans `[0;nb_val_1]`,   les complexités de `join_hash` et `join_merge` doivent avoir  des majorants du même ordre de grandeur, qui vont différer par une constante. On remarque que `join_hash` est plus rapide : les tests d'égalité sont remplacés par des calculs de `hash` dans la phase de construction de l'index puis dans la phase de scan de `sample_2`.\n",
    "\n",
    "Si on augmente la taille de `nb_val_1`, on augmente la valeur  de `n`  le nombre maximal de tuple de l'échantillon `sample_1` qui ont la même valeur sur l'attribut 1 et la complexité de `join_hash` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_1 = 100, nb_val_1=100, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.06487639984698035 ms\n",
      "Temps pour une exécution des tris            : 0.02604300098028034 ms\n",
      "Temps pour une exécution de join_merge       : 0.07847550004953519 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.03723859990714118 ms\n",
      "Temps pour une exécution des tris            : 0.026650400832295418 ms\n",
      "Temps pour une exécution de join_merge       : 0.02976120013045147 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0360557998646982 ms\n",
      "Temps pour une exécution des tris            : 0.026795199664775282 ms\n",
      "Temps pour une exécution de join_merge       : 0.03088430021307431 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.03377699977136217 ms\n",
      "Temps pour une exécution des tris            : 0.026375000743428245 ms\n",
      "Temps pour une exécution de join_merge       : 0.025830799859249964 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.040561199421063066 ms\n",
      "Temps pour une exécution des tris            : 0.029540999821620062 ms\n",
      "Temps pour une exécution de join_merge       : 0.02553040030761622 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.6191058004333172 ms\n",
      "Temps pour une exécution des tris            : 0.1387502001307439 ms\n",
      "Temps pour une exécution de join_merge       : 0.597524999466259 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.34500290057621896 ms\n",
      "Temps pour une exécution des tris            : 0.15783260023454204 ms\n",
      "Temps pour une exécution de join_merge       : 0.08646869973745197 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.2834549995895941 ms\n",
      "Temps pour une exécution des tris            : 0.145574700582074 ms\n",
      "Temps pour une exécution de join_merge       : 0.027901700377697125 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.3735779995622579 ms\n",
      "Temps pour une exécution des tris            : 0.18082390015479177 ms\n",
      "Temps pour une exécution de join_merge       : 0.024466800095979124 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.38672630034852773 ms\n",
      "Temps pour une exécution des tris            : 0.18516540003474802 ms\n",
      "Temps pour une exécution de join_merge       : 0.032212699443334714 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 5.260058600106277 ms\n",
      "Temps pour une exécution des tris            : 1.4212722999218386 ms\n",
      "Temps pour une exécution de join_merge       : 6.1743347003357485 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 4.113573199720122 ms\n",
      "Temps pour une exécution des tris            : 2.2972315004153643 ms\n",
      "Temps pour une exécution de join_merge       : 0.6502240998088382 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 6.101142900297418 ms\n",
      "Temps pour une exécution des tris            : 3.2732343992393 ms\n",
      "Temps pour une exécution de join_merge       : 0.07638820025022142 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 4.639060900080949 ms\n",
      "Temps pour une exécution des tris            : 2.5504425997496583 ms\n",
      "Temps pour une exécution de join_merge       : 0.030049400083953515 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 4.018960300163599 ms\n",
      "Temps pour une exécution des tris            : 1.4841213000181597 ms\n",
      "Temps pour une exécution de join_merge       : 0.02376909978920594 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.051790599536616355 ms\n",
      "Temps pour une exécution des tris            : 0.027344100089976564 ms\n",
      "Temps pour une exécution de join_merge       : 0.030239300394896418 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.04101209997315891 ms\n",
      "Temps pour une exécution des tris            : 0.02597880011308007 ms\n",
      "Temps pour une exécution de join_merge       : 0.05230169990682043 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.046504799684043974 ms\n",
      "Temps pour une exécution des tris            : 0.025738399563124403 ms\n",
      "Temps pour une exécution de join_merge       : 0.026717899891082197 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.03407120020710863 ms\n",
      "Temps pour une exécution des tris            : 0.02566399925854057 ms\n",
      "Temps pour une exécution de join_merge       : 0.02660309983184561 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.03128569951513782 ms\n",
      "Temps pour une exécution des tris            : 0.025719400582602248 ms\n",
      "Temps pour une exécution de join_merge       : 0.0235652994888369 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.31596159969922155 ms\n",
      "Temps pour une exécution des tris            : 0.13733349987887777 ms\n",
      "Temps pour une exécution de join_merge       : 0.2529874996980652 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.383243800024502 ms\n",
      "Temps pour une exécution des tris            : 0.1808149005228188 ms\n",
      "Temps pour une exécution de join_merge       : 0.3603009994549211 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.33404540008632466 ms\n",
      "Temps pour une exécution des tris            : 0.14185040054144338 ms\n",
      "Temps pour une exécution de join_merge       : 0.050396400183672085 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.27751840025302954 ms\n",
      "Temps pour une exécution des tris            : 0.21151999972062185 ms\n",
      "Temps pour une exécution de join_merge       : 0.03829439956462011 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.2895705998525955 ms\n",
      "Temps pour une exécution des tris            : 0.13856590012437664 ms\n",
      "Temps pour une exécution de join_merge       : 0.024181399930967018 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 3.331283599982271 ms\n",
      "Temps pour une exécution des tris            : 1.347142799932044 ms\n",
      "Temps pour une exécution de join_merge       : 2.5304876995505765 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 5.093903499800945 ms\n",
      "Temps pour une exécution des tris            : 1.438574599887943 ms\n",
      "Temps pour une exécution de join_merge       : 2.794264300609939 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 4.075662000104785 ms\n",
      "Temps pour une exécution des tris            : 1.5487796008528676 ms\n",
      "Temps pour une exécution de join_merge       : 0.27703800005838275 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 4.007355099747656 ms\n",
      "Temps pour une exécution des tris            : 1.8270013999426737 ms\n",
      "Temps pour une exécution de join_merge       : 0.06192840010044165 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 4.116562200215412 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour une exécution des tris            : 1.8232575996080413 ms\n",
      "Temps pour une exécution de join_merge       : 0.03469880030024797 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.45617490031872876 ms\n",
      "Temps pour une exécution des tris            : 0.1453621000109706 ms\n",
      "Temps pour une exécution de join_merge       : 0.60511050032801 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.14470919995801523 ms\n",
      "Temps pour une exécution des tris            : 0.14033539919182658 ms\n",
      "Temps pour une exécution de join_merge       : 0.2831247002177406 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.09837830002652481 ms\n",
      "Temps pour une exécution des tris            : 0.13816150021739304 ms\n",
      "Temps pour une exécution de join_merge       : 0.26935109999612905 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.10672279968275689 ms\n",
      "Temps pour une exécution des tris            : 0.14160569990053773 ms\n",
      "Temps pour une exécution de join_merge       : 0.2502136994735338 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.09154129948001355 ms\n",
      "Temps pour une exécution des tris            : 0.1507819994003512 ms\n",
      "Temps pour une exécution de join_merge       : 0.2617496000311803 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 2.7390634000767022 ms\n",
      "Temps pour une exécution des tris            : 0.2880640000512358 ms\n",
      "Temps pour une exécution de join_merge       : 3.20519550004974 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.7129389996407554 ms\n",
      "Temps pour une exécution des tris            : 0.24956220004241914 ms\n",
      "Temps pour une exécution de join_merge       : 0.5651311999827158 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.37184850007179193 ms\n",
      "Temps pour une exécution des tris            : 0.2481407005689107 ms\n",
      "Temps pour une exécution de join_merge       : 0.31845760022406466 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.3727868002897594 ms\n",
      "Temps pour une exécution des tris            : 0.2624309003294911 ms\n",
      "Temps pour une exécution de join_merge       : 0.26431730002514087 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.3509242000291124 ms\n",
      "Temps pour une exécution des tris            : 0.2648011999554001 ms\n",
      "Temps pour une exécution de join_merge       : 0.24823040002956986 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 27.155509700241964 ms\n",
      "Temps pour une exécution des tris            : 1.639196800533682 ms\n",
      "Temps pour une exécution de join_merge       : 34.2055133995018 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 5.75106130054337 ms\n",
      "Temps pour une exécution des tris            : 1.5222032998281065 ms\n",
      "Temps pour une exécution de join_merge       : 3.2159856004000176 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 3.879374800453661 ms\n",
      "Temps pour une exécution des tris            : 1.5898835001280531 ms\n",
      "Temps pour une exécution de join_merge       : 0.5065066005045082 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 3.275956099969335 ms\n",
      "Temps pour une exécution des tris            : 1.5483734001463745 ms\n",
      "Temps pour une exécution de join_merge       : 0.30505199974868447 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 3.1860793998930603 ms\n",
      "Temps pour une exécution des tris            : 1.5594589000102133 ms\n",
      "Temps pour une exécution de join_merge       : 0.24494639947079122 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.11784110029111616 ms\n",
      "Temps pour une exécution des tris            : 0.13950759966974147 ms\n",
      "Temps pour une exécution de join_merge       : 0.06898319988977164 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.13103469973430037 ms\n",
      "Temps pour une exécution des tris            : 0.18614979926496744 ms\n",
      "Temps pour une exécution de join_merge       : 0.4559950997645501 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.12598169996635988 ms\n",
      "Temps pour une exécution des tris            : 0.17242219983018003 ms\n",
      "Temps pour une exécution de join_merge       : 0.25544480013195425 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.09841599967330694 ms\n",
      "Temps pour une exécution des tris            : 0.1509058005467523 ms\n",
      "Temps pour une exécution de join_merge       : 0.38109529996290803 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.12569370010169223 ms\n",
      "Temps pour une exécution des tris            : 0.2075202006381005 ms\n",
      "Temps pour une exécution de join_merge       : 0.33142590036732145 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.6011250996380113 ms\n",
      "Temps pour une exécution des tris            : 0.24863529979484156 ms\n",
      "Temps pour une exécution de join_merge       : 0.5884828999114688 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.7507554000767414 ms\n",
      "Temps pour une exécution des tris            : 0.25456559960730374 ms\n",
      "Temps pour une exécution de join_merge       : 0.8983296997030266 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.44409620022634044 ms\n",
      "Temps pour une exécution des tris            : 0.25170199951389804 ms\n",
      "Temps pour une exécution de join_merge       : 0.3294841997558251 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.3497962999972515 ms\n",
      "Temps pour une exécution des tris            : 0.260749900189694 ms\n",
      "Temps pour une exécution de join_merge       : 0.27960529987467453 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.33536589980940334 ms\n",
      "Temps pour une exécution des tris            : 0.25453330017626286 ms\n",
      "Temps pour une exécution de join_merge       : 0.26510809984756634 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 5.069525100407191 ms\n",
      "Temps pour une exécution des tris            : 1.41007870042813 ms\n",
      "Temps pour une exécution de join_merge       : 5.813185399892973 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 5.995756600168534 ms\n",
      "Temps pour une exécution des tris            : 1.5861190004216041 ms\n",
      "Temps pour une exécution de join_merge       : 6.4618165997671895 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 4.59357449944946 ms\n",
      "Temps pour une exécution des tris            : 1.6974666999885812 ms\n",
      "Temps pour une exécution de join_merge       : 0.840031400002772 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 3.301609399932204 ms\n",
      "Temps pour une exécution des tris            : 1.648029799980577 ms\n",
      "Temps pour une exécution de join_merge       : 0.3342547999636736 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 3.08712789992569 ms\n",
      "Temps pour une exécution des tris            : 1.5869373004534282 ms\n",
      "Temps pour une exécution de join_merge       : 0.24740090011619031 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 3.768412100180285 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour une exécution des tris            : 1.3631802001327742 ms\n",
      "Temps pour une exécution de join_merge       : 5.582648400013568 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.924339699849952 ms\n",
      "Temps pour une exécution des tris            : 1.3591831993835513 ms\n",
      "Temps pour une exécution de join_merge       : 2.7109239003038965 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.7836624004994519 ms\n",
      "Temps pour une exécution des tris            : 1.3186665004468523 ms\n",
      "Temps pour une exécution de join_merge       : 2.6026965999335516 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.604902500344906 ms\n",
      "Temps pour une exécution des tris            : 1.321973800077103 ms\n",
      "Temps pour une exécution de join_merge       : 2.509575799922459 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.637950400414411 ms\n",
      "Temps pour une exécution des tris            : 1.3473261999024544 ms\n",
      "Temps pour une exécution de join_merge       : 2.53759440020076 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 23.58463459968334 ms\n",
      "Temps pour une exécution des tris            : 1.4261022995924577 ms\n",
      "Temps pour une exécution de join_merge       : 33.74509630011744 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 4.761795000376878 ms\n",
      "Temps pour une exécution des tris            : 1.6852644992468413 ms\n",
      "Temps pour une exécution de join_merge       : 6.030181000096491 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 1.7299813996942248 ms\n",
      "Temps pour une exécution des tris            : 1.421321200177772 ms\n",
      "Temps pour une exécution de join_merge       : 2.9202153004007414 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.9159861001535319 ms\n",
      "Temps pour une exécution des tris            : 1.4444881002418697 ms\n",
      "Temps pour une exécution de join_merge       : 2.481773500039708 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.8629165997263044 ms\n",
      "Temps pour une exécution des tris            : 1.4221849000023212 ms\n",
      "Temps pour une exécution de join_merge       : 2.47461720064166 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 229.47369320027065 ms\n",
      "Temps pour une exécution des tris            : 2.6395918997877743 ms\n",
      "Temps pour une exécution de join_merge       : 301.4489496999886 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 28.742688900092617 ms\n",
      "Temps pour une exécution des tris            : 2.770607001002645 ms\n",
      "Temps pour une exécution de join_merge       : 31.39510920009343 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 8.274563300074078 ms\n",
      "Temps pour une exécution des tris            : 2.7676330995745957 ms\n",
      "Temps pour une exécution de join_merge       : 5.18927570010419 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 4.846929400082445 ms\n",
      "Temps pour une exécution des tris            : 2.8469597004004754 ms\n",
      "Temps pour une exécution de join_merge       : 2.8832967997004744 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 4.5382069001789205 ms\n",
      "Temps pour une exécution des tris            : 3.2594930999039207 ms\n",
      "Temps pour une exécution de join_merge       : 3.1852040003286675 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 1.3209472999733407 ms\n",
      "Temps pour une exécution des tris            : 1.619500699598575 ms\n",
      "Temps pour une exécution de join_merge       : 0.5859840995981358 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 1.1345613005687483 ms\n",
      "Temps pour une exécution des tris            : 1.4175127005728427 ms\n",
      "Temps pour une exécution de join_merge       : 3.567012699932093 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.8253239997429773 ms\n",
      "Temps pour une exécution des tris            : 1.5154007996898144 ms\n",
      "Temps pour une exécution de join_merge       : 4.4166610001411755 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.6578977998287883 ms\n",
      "Temps pour une exécution des tris            : 1.5208600991172716 ms\n",
      "Temps pour une exécution de join_merge       : 2.819034999993164 ms\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.7006506995821837 ms\n",
      "Temps pour une exécution des tris            : 1.464032299554674 ms\n",
      "Temps pour une exécution de join_merge       : 2.5233020998712163 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 2.744745500240242 ms\n",
      "Temps pour une exécution des tris            : 1.5225816998281516 ms\n",
      "Temps pour une exécution de join_merge       : 3.1108752002182882 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 4.5438733002811205 ms\n",
      "Temps pour une exécution des tris            : 1.54658199971891 ms\n",
      "Temps pour une exécution de join_merge       : 6.025036500068381 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 1.3398552997387014 ms\n",
      "Temps pour une exécution des tris            : 1.5391385000839364 ms\n",
      "Temps pour une exécution de join_merge       : 2.8694607004581485 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.9594348004611675 ms\n",
      "Temps pour une exécution des tris            : 1.5179875001194887 ms\n",
      "Temps pour une exécution de join_merge       : 2.5539729998854455 ms\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.9020794001116883 ms\n",
      "Temps pour une exécution des tris            : 1.5248003997839987 ms\n",
      "Temps pour une exécution de join_merge       : 2.524418899702141 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 27.960565800458426 ms\n",
      "Temps pour une exécution des tris            : 2.7408059002482332 ms\n",
      "Temps pour une exécution de join_merge       : 34.82852019951679 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 33.37396319984691 ms\n",
      "Temps pour une exécution des tris            : 2.8797803999623284 ms\n",
      "Temps pour une exécution de join_merge       : 41.10947480003233 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 9.151948199723847 ms\n",
      "Temps pour une exécution des tris            : 2.8315551004197914 ms\n",
      "Temps pour une exécution de join_merge       : 5.742274500516942 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 4.876633499952732 ms\n",
      "Temps pour une exécution des tris            : 3.0162585993821267 ms\n",
      "Temps pour une exécution de join_merge       : 2.8134815001976676 ms\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 4.080326200346462 ms\n",
      "Temps pour une exécution des tris            : 3.27711000063573 ms\n",
      "Temps pour une exécution de join_merge       : 2.661350300331833 ms\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for size_2 in [10**k for k in range(2, 5)]:    \n",
    "    for nb_val_2 in [10**k for k in range(2, 4)]:\n",
    "        for size_1 in [10**k for k in range(2, 5)]: \n",
    "            for nb_val_1 in [10**k for k in range(2, 7)]:\n",
    "                print(f\"size_1 = {size_1}, nb_val_1={nb_val_1}, size_2 = {size_2},  nb_val_2={nb_val_2}\")\n",
    "                benchmark(bench_loop=False, size_1 = size_1, nb_val_1 =nb_val_1  ,size_2 = size_2, nb_val_2 = nb_val_2, nb_repeat = 10)\n",
    "                print('-'* 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que `join_merge` est beaucoup plus rapide lorsque `size_1` est beaucoup plus grand que `size_2` et d'autant plus que `nb_val_1` est grand (et donc la longueur maximale d'une liste stockée dans l'index/table de hash)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE (POUR ALLER PLUS LOIN)** : même question que précédemment, mais cette fois si il faut trouver un cas qui est favorable à `join_nested_loop` et dévaforable aux deux autres. _Indice_ faites en sorte que la jointure soit aussi grosse que le produit cartésien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour une exécution de join_nested_loop : 1.298631440004101 ms\n",
      "Temps pour une exécution de join_hash        : 1.5078773900313536 ms\n",
      "Temps pour une exécution des tris            : 0.02175726003770251 ms\n",
      "Temps pour une exécution de join_merge       : 2.46137122005166 ms\n"
     ]
    }
   ],
   "source": [
    "#il suffit de prende la même constante pour les attributs de jointure\n",
    "#en choisissant aléatoirement sa valeur dans un ensemble de taille 1\n",
    "\n",
    "benchmark(size_1=100, nb_val_1=1,\n",
    "              size_2=100, nb_val_2=1,\n",
    "              nb_repeat=100,\n",
    "              bench_loop=True, bench_hash=True, bench_merge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparer l'exécution dans Python à celle native dans SQLite\n",
    "-------------------------------------------------------------\n",
    "\n",
    "Maintenant, on va comparer la performance de ces implantations Python face aux algorithmes jointures de SQLite (qui est écrit en C). Pour cela on va comparer les deux approches suivantes :\n",
    "\n",
    "* **Approche A : jointure en SQLite**, on exécute la requête `SELECT * FROM table1 JOIN table2 ON table1.val == table2.val` puis (depuis Python) on récupère l'intégralité du résultat, c'est la fonction `join_python()`\n",
    "* **Approche B : jointure en Python**, on exécute la requête `SELECT * FROM table1` et on stocke son résultat dans un tableau, de même pour `SELECT * FROM table2` puis on utilise un des algorithmes précedents pour faire le calcul de jointure et enfin on renvoie le résultat, c'est la fonction `join_sqlite()`\n",
    "\n",
    "\n",
    "**EXERCICE** : créer une nouvelle base de données nommée `join_algorithms_versus_sqlite3.db` et exécuter le script SQL `join_algorithms_schema.sql` pour créer le schéma *et* peupler la base avec un jeu de données similaire à celui du benchmark de l'exercice précédent.\n",
    "\n",
    "**EXERCICE** : en vous inspirant du code fourni, compléter les fonctions `join_python()` et `join_sqlite()` du programme [`join_algorithms_versus_sqlite3.py`](join_algorithms_versus_sqlite3.py) et observer les temps d'exécution.\n",
    "\n",
    " **EXERCICE** : ensuite, exécuter la requête de jointure directement dans `SQLite3` en activant activant le chronométrage avec `.timer on` depuis l'interpréteur ligne de commande ou depuis _DB Browser for SQLite_ (le temps est affiché en bas de la fenêtre d'exécution). Vous devriez avoir une différence _de plusieurs ordre de magnitude entre les deux_ : comment l'expliquer ?\n",
    "\n",
    " **EXERCICE** : reprendre la comparaison mais cette fois avec la requête `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val`. Ici, `join_python()` renverra _la longueur du tableau_, comme par exemple `len(join_hash(table1, 1, table2, 0))` pour l'algorithme de jointure par hash. Comparer les temps d'exécution, une différence _de plusieurs ordre de magnitude doit les séparer_  : comment l'expliquer ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.displaycon = False\n",
    "%config SqlMagic.autolimit = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql sqlite:///join_algorithms_versus_sqlite3.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "PRAGMA foreign_keys=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "-- POUR CHARGER DANS SQLite3 : \n",
    "-- sqlite3 join_algorithms_versus_sqlite3.db\n",
    "-- .timer on\n",
    "-- .read join_algorithms_schema.sql\n",
    "\n",
    "drop table if exists Table1;\n",
    "drop table if exists Table2;\n",
    "\n",
    "-- Un schéma très simple, qui reprend le benchmark des 3 algos en Python qu'on rappelle ici :\n",
    "--\n",
    "-- def benchmark(size_1=1000, nb_val_1=100,\n",
    "--               size_2=1000, nb_val_2=100,\n",
    "--               nb_repeat=100,\n",
    "--               bench_loop=True, bench_hash=True, bench_merge=True):\n",
    "--\n",
    "--    sample_1 = [(i, randrange(nb_val_1)) for i in range(size_1)]\n",
    "--    sample_2 = [(randrange(nb_val_2), 'A'+str(j)) for j in range(size_2)]\n",
    "\n",
    "-- Activation des clefs étrangères\n",
    "PRAGMA foreign_keys=1; \n",
    "\n",
    "create table Table1(\n",
    "    idA INTEGER PRIMARY KEY,    -- un ID entier\n",
    "    val INTEGER     -- l'attribut de jointure\n",
    ");\n",
    "\n",
    "create table Table2(\n",
    "    val INTEGER,  -- pas  REFERENCES Table1(val), mais ca ne changerait rien\n",
    "    idB INTEGER\n",
    ");\n",
    "\n",
    "\n",
    " -- On va remplir les tables avec 10.000 lignes pour A et autant pour B\n",
    "\n",
    "--pour générer récursivement tous les entiers entre 1 et 10000\n",
    "--voir https://sqlpro.developpez.com/cours/sqlserver/cte-recursives/\n",
    "WITH RECURSIVE data_1(val) AS (\n",
    "  SELECT 1\n",
    "  UNION ALL\n",
    "  SELECT val+1\n",
    "  FROM data_1\n",
    "  WHERE val+1 <= 10000\n",
    ") \n",
    "INSERT INTO Table1 \n",
    "  SELECT  val,\n",
    "          ABS(RANDOM() % 1000)   --entiers aléatoires entre 0 et 1000\n",
    "  FROM data_1;\n",
    "\n",
    "WITH RECURSIVE data_2(val) AS (\n",
    "  SELECT 1\n",
    "  UNION ALL\n",
    "  SELECT val+1\n",
    "  FROM data_2\n",
    "  WHERE val+1 <= 10000\n",
    ") \n",
    "INSERT INTO Table2\n",
    "  SELECT  ABS(RANDOM() % 1000),\n",
    "          'A' || val\n",
    "  FROM data_2;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Un module pour comparer la performance des jointures 'à la main'\n",
    "    en Python face à ceux en C bien choisis de SQLite3\"\"\"\n",
    "\n",
    "import sqlite3\n",
    "import logging\n",
    "#voir https://docs.python.org/3/howto/logging.html\n",
    "from timeit import timeit\n",
    "#from join_algorithms import join_nested_loop, join_hash, join_merge\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "DB_FILE = 'join_algorithms_versus_sqlite3.db'\n",
    "\n",
    "def join_algorithms_versus_sqlite3():\n",
    "    \"\"\"Fonction de comparaison\"\"\"\n",
    "    try:\n",
    "        connection = sqlite3.connect(DB_FILE)\n",
    "\n",
    "        # Pour avoir des ditcionnaires et non des tuples dans le résultat\n",
    "        # see https://docs.python.org/3.6/library/sqlite3.html#sqlite3.Row\n",
    "        # connection.row_factory = sqlite3.Row\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        logging.debug(\"You are connected to - %s\", DB_FILE)\n",
    "        cursor.execute(\"SELECT sqlite_version() as version;\")\n",
    "        record = cursor.fetchone()\n",
    "        logging.debug(tuple(record))\n",
    "\n",
    "        def join_python(jointure):\n",
    "            cursor.execute(\"SELECT * FROM table1;\")\n",
    "            table1 = cursor.fetchall()\n",
    "            #print(table1[:10])\n",
    "            cursor.execute(\"SELECT * FROM table2;\")\n",
    "            table2 = cursor.fetchall()\n",
    "            jointure(table1, 1,table2, 0)\n",
    "            \n",
    "        def join_sqlite():\n",
    "            cursor.execute(\"SELECT * FROM table1 JOIN table2 ON table1.val == table2.val;\")\n",
    "            record = cursor.fetchone()\n",
    "            \n",
    "        for jointure in [join_hash, join_merge]:\n",
    "            time_join_python = timeit(lambda : join_python(jointure), number=100)\n",
    "            logging.info(f'Temps pour une jointure {jointure.__name__} côté Python : %f', time_join_python)\n",
    "\n",
    "        time_join_sqlite = timeit(join_sqlite, number=100)\n",
    "        logging.info('Temps pour une jointure côté Sqlite3 : %f', time_join_sqlite)\n",
    "\n",
    "    except (sqlite3.Error) as error:\n",
    "        logging.error(\"Error while connecting to sqlite3: %s\", error)\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            logging.debug(\"Sqlite3 connection is closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Temps pour une jointure join_hash côté Python : 4.994858\n",
      "INFO:root:Temps pour une jointure join_merge côté Python : 1.740257\n",
      "INFO:root:Temps pour une jointure côté Sqlite3 : 0.879510\n"
     ]
    }
   ],
   "source": [
    "join_algorithms_versus_sqlite3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **EXERCICE** : ensuite, exécuter la requête de jointure directement dans `SQLite3` en activant activant le chronométrage avec `.timer on` depuis l'interpréteur ligne de commande ou depuis _DB Browser for SQLite_ (le temps est affiché en bas de la fenêtre d'exécution). Vous devriez avoir une différence _de plusieurs ordre de magnitude entre les deux_ : comment l'expliquer ?\n",
    " \n",
    "* sqlite3 CLI :\n",
    "\n",
    "![sqlite3-CLI](time_sqlite3.png)\n",
    "\n",
    "* sqlite DBBrowser :\n",
    "\n",
    "![sqlite DBBrowser](time_dbbrowser.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE** : reprendre la comparaison mais cette fois avec la requête `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val`. Ici, `join_python()` renverra _la longueur du tableau_, comme par exemple `len(join_hash(table1, 1, table2, 0))` pour l'algorithme de jointure par hash. Comparer les temps d'exécution, une différence _de plusieurs ordre de magnitude doit les séparer_  : comment l'expliquer ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_algorithms_versus_sqlite3_V2():\n",
    "    \"\"\"Fonction de comparaison\"\"\"\n",
    "    try:\n",
    "        connection = sqlite3.connect(DB_FILE)\n",
    "\n",
    "        # Pour avoir des ditcionnaires et non des tuples dans le résultat\n",
    "        # see https://docs.python.org/3.6/library/sqlite3.html#sqlite3.Row\n",
    "        # connection.row_factory = sqlite3.Row\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        logging.debug(\"You are connected to - %s\", DB_FILE)\n",
    "        cursor.execute(\"SELECT sqlite_version() as version;\")\n",
    "        record = cursor.fetchone()\n",
    "        logging.debug(tuple(record))\n",
    "\n",
    "        def join_python(jointure):\n",
    "            cursor.execute(\"SELECT * FROM table1;\")\n",
    "            table1 = cursor.fetchall()\n",
    "            #print(table1[:10])\n",
    "            cursor.execute(\"SELECT * FROM table2;\")\n",
    "            table2 = cursor.fetchall()           \n",
    "            return len( jointure(table1, 1,table2, 0))\n",
    "            \n",
    "        def join_sqlite():\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val;\")\n",
    "            record = cursor.fetchone()\n",
    "            return record\n",
    "        \n",
    "                \n",
    "        for jointure in [join_hash, join_merge]:\n",
    "            time_join_python = timeit(lambda : join_python(jointure), number=100)\n",
    "            logging.info(f'Temps pour une jointure {jointure.__name__} côté Python : %f', time_join_python)\n",
    "            \n",
    "        time_join_sqlite = timeit(join_sqlite, number=100)\n",
    "        logging.info('Temps pour une jointure côté Sqlite3 : %f', time_join_sqlite)\n",
    "\n",
    "        \n",
    "\n",
    "    except (sqlite3.Error) as error:\n",
    "        logging.error(\"Error while connecting to sqlite3: %s\", error)\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            logging.debug(\"Sqlite3 connection is closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Temps pour une jointure join_hash côté Python : 4.893128\n",
      "INFO:root:Temps pour une jointure join_merge côté Python : 1.698852\n",
      "INFO:root:Temps pour une jointure côté Sqlite3 : 1.610413\n"
     ]
    }
   ],
   "source": [
    "join_algorithms_versus_sqlite3_V2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas, on a un facteur 100 entre l'exécution en ligne de commandes et l'exécution à travers un programme Python. La requête `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val;` est plus lente que  `SELECT * FROM table1 JOIN table2 ON table1.val == table2.val;` depuis un programme Python si on l'exécute dans `sqlite3` d'abord et similaire si on fait la jointure après avec nos fonctions maisons. On observe au contraire que la  requête  `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val;`  s'exécute plus rapidement en ligne de commande.\n",
    "\n",
    "\n",
    "* sqlite3 CLI :\n",
    "\n",
    "![sqlite3-CLI](timer2_sqlite3.png)\n",
    "\n",
    "* sqlite DBBrowser :\n",
    "\n",
    "![sqlite DBBrowser](timer2_sqliteDBBrowser.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EXERCICE (FACULTATIF ET OUVERT) :__\n",
    "\n",
    "Conclure en formulant quelques bonnes pratiques de l'accès à une base de données via un programme (Python).\n",
    "\n",
    "\n",
    "* Bonne pratique 1 : Faire les opérations SQL (jointures) dans SQLite et non dans Python\n",
    "* Bonne pratique 2 : se méfier des tests en ligne de commandes qui peuvent donner de mauvaises indications de performances (voir exercice précédent avec les requêtes `SELECT * FROM table1 JOIN table2 ON table1.val == table2.val;` et `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val;`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
