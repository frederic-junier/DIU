{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIU bloc 4 : \"Bases de données : création de schémas et normalisation\" : TP sur les algorithmes de jointure\n",
    "====================================================\n",
    "\n",
    "Dans ce TP, on va s'intéresser **aux algorithmes de jointures**, c'est-à-dire aux algorithmes exécutés par les moteurs des SGBDs quand ils traduisent des requêtes comme la suivante :\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM table1 JOIN table2 ON table1.attr1 == table2.attr2\n",
    "```\n",
    "\n",
    "Il existe plusieurs algorithmes de jointure et l'optimiseur de requêtes du SGBD va tâcher de choisir le _meilleur_, vis-à-vis de statistiques sur les données et surtout des **index** disponibles sur les tables.\n",
    "Le but du TP est ainsi de comprendre ces algorithmes fondamentaux et de les comparer entre eux puis de les comparer face à deux de SQLite 3.\n",
    "\n",
    "**Remarque** la comparaison de performance (_benchmark_) est un exercice complexe car de nombreux paramètres très différents contribuent à la performance finale (matériel, OS, I/O disques ou d'affichage, efficacité de la compilation/interprétation du langage de programmation, caches, temps d'initialisation etc.).\n",
    "\n",
    "Implanter les algorithmes classiques de jointure en Python\n",
    "-----------------------------------------------------------\n",
    "\n",
    "Le fichier [`join_algorithms.py`](join_algorithms.py) contient le squelette à remplir pour les trois algorithmes, à savoir _nested loop_, _hash join_ et _merge join_. Ces algorithmes font la même chose et ont la même signature `def algo(table1, attr1, table2, attr2):` :\n",
    "\n",
    "* `table1` et `table2` sont des listes (Python) de tuples (Python). Il n'y a pas de garanties d'ordre sur ces listes;\n",
    "* `attr1` (resp. `attr2`) est _l'indice_ (entier) de l'attribut de `table1` (resp. de `tablee`) sur lequel on fait la jointure;\n",
    "* ces algorithmes retournent tous une liste de tuples, comme l'aurait fait la requête SQL.\n",
    "\n",
    "\n",
    "Le fichier [`join_algorithms_test.py`](join_algorithms_test.py) donne un exemple d'entrées et de résultats attendus.\n",
    "\n",
    "**EXERCICE** : compléter la fonction `join_nested_loop` et tester votre implantation avec `pytest-3` et les tests fournis.\n",
    "\n",
    "**EXERCICE** : compléter la fonction `join_hash` et tester votre implantation avec `pytest-3` et les tests fournis.\n",
    "\n",
    "**EXERCICE (POUR ALLER PLUS LOIN)** : compléter la fonction `join_merge` et tester votre implantations avec `pytest-3` et les tests fournis. Vous n'êtes pas obligé de faire cet exercice pour passer à la suite.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Un module pour illustrer les principaux alogorithmes de jointure en Python\n",
    "    On ne gèrera ici le cas nominal d'utilisation des fonctions où\n",
    "    attr1 et attr2 sont LES INDEX EXISTANT des attributs\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from random import randrange\n",
    "from timeit import timeit\n",
    "from collections import defaultdict\n",
    "# voir https://docs.python.org/3.6/library/collections.html#collections.defaultdict\n",
    "# est utile pour le hash join en initialisant à la liste vide\n",
    "\n",
    "\n",
    "\n",
    "def join_nested_loop(table1, attr1, table2, attr2):\n",
    "    \"\"\"L'algorithme naif de jointure par deux boucles imbriquées :\n",
    "       pour chaque tuple de tabl1, on va lire toute la table2\n",
    "       et produire un résultat à chaque fois que tup1[attr1] == tup2[attr2]\n",
    "       https://en.wikipedia.org/wiki/Nested_loop_join\n",
    "\n",
    "       CONTRAT: pas de préconditions sur les entrées\"\"\"\n",
    "    return [t1 + t2 for t1 in table1 for t2 in table2 if t1[attr1] == t2[attr2]]\n",
    "\n",
    "\n",
    "def join_hash(table1, attr1, table2, attr2):\n",
    "    \"\"\"L'algorithme de jointure en deux étapes :\n",
    "          1. construction d'un index sur table1 via un dictionnaire :\n",
    "             à chaque valeur de attr1, on associe **la liste** des index des\n",
    "             tuples de table1 qui ont cette valeur\n",
    "          2. scan de table2 :\n",
    "             pour chaque tuple, on va chercher AVEC L'INDEX la liste\n",
    "             des tuples de table1 qui on la valeur de attr2 pour attr1\n",
    "             On fait attention aux collisions de hash et on ajoute les\n",
    "             tuples au résultat.\n",
    "\n",
    "        https://en.wikipedia.org/wiki/Hash_join#Classic_hash_join\n",
    "\n",
    "        CONTRAT: pas de préconditions sur les entrées\"\"\"\n",
    "    res = []\n",
    "    index = dict()\n",
    "    for k, t1 in enumerate(table1):\n",
    "        val = t1[attr1]\n",
    "        #pour gérer les collisions de hash\n",
    "        #on stocke dans index[val] une liste de listes dont le premier élément est t1[attr1]\n",
    "        #et dont les éléments suivants sont les index des tuples de table1 qui ont même\n",
    "        #même valeur val pour l'attribut d'index attr1\n",
    "        if val in index:\n",
    "            for l in  index[val]:\n",
    "                if l[0] == val:\n",
    "                    l.append(k)\n",
    "                    break\n",
    "        else:\n",
    "            index[val] = [[val, k]]\n",
    "    for t2 in table2:\n",
    "        val = t2[attr2]\n",
    "        if val in index:\n",
    "            for l in index[val]:\n",
    "                if l[0] == val:\n",
    "                    for k in l[1:]:\n",
    "                        res.append(table1[k] + t2)\n",
    "    return res\n",
    "\n",
    "def join_hash2(table1, attr1, table2, attr2):\n",
    "    \"\"\"L'algorithme de jointure en deux étapes :\n",
    "          1. construction d'un index sur table1 via un dictionnaire :\n",
    "             à chaque valeur de attr1, on associe **la liste** des index des\n",
    "             tuples de table1 qui ont cette valeur\n",
    "          2. scan de table2 :\n",
    "             pour chaque tuple, on va chercher AVEC L'INDEX la liste\n",
    "             des tuples de table1 qui on la valeur de attr2 pour attr1\n",
    "             On fait attention aux collisions de hash et on ajoute les\n",
    "             tuples au résultat.\n",
    "\n",
    "        https://en.wikipedia.org/wiki/Hash_join#Classic_hash_join\n",
    "\n",
    "        CONTRAT: pas de préconditions sur les entrées\"\"\"\n",
    "    # en fait, comme les dictionnaires Python utilisent des tables de hash\n",
    "    # il suffit d'un dictionnaire qui sert d'index (un peu comme pour le\n",
    "    # décorateur memoize) pour implémenter cet algorithme\n",
    "    #hash_table = defaultdict(list)\n",
    "    #pass\n",
    "    res = []\n",
    "    index = dict()\n",
    "    for k, t1 in enumerate(table1):\n",
    "        val = t1[attr1]\n",
    "        #pour gérer les collisions de hash\n",
    "        #on stocke dans index[val] une liste de listes dont le premier élément est t1[attr1]\n",
    "        #et dont les éléments suivants sont les index des tuples de table1 qui ont même\n",
    "        #même valeur val pour l'attribut d'index attr1\n",
    "        if val in index:\n",
    "            index[val].append(k)\n",
    "        else:\n",
    "            index[val] = [k]\n",
    "    for t2 in table2:\n",
    "        val = t2[attr2]\n",
    "        if val in index:\n",
    "            for k in index[val]:\n",
    "                res.append(table1[k] + t2)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def join_merge(table1, attr1, table2, attr2):\n",
    "    \"\"\"L'algorithme de jointure sort-merge qui s'appuie sur des tables SUPPOSEES TRIEES\n",
    "       https://en.wikipedia.org/wiki/Sort-merge_join\n",
    "       Son principe est assez similaire à l'étape \"merge\" du merge sort\n",
    "       https://en.wikipedia.org/wiki/Merge_sort\n",
    "\n",
    "       On avance en // sur table1 et table2 avec 2 index ind1 et ind2\n",
    "        - si table1[ind1][attr1] est avant table2[ind2][attr2]\n",
    "          on incrémente ind1\n",
    "        - si table2[ind2][attr2] est avant table1[ind1][attr1]\n",
    "          on incrémente ind2\n",
    "        - si les tuples sur lesquels on se trouve respectent la condition\n",
    "                table1[ind1][attr1] == table2[ind2][attr2]\n",
    "           alors avec une boucle locale, on va chercher tous les tuples\n",
    "           de table2 satisfont la condition et ajouter au résultat.\n",
    "           ensuite on incrémente ind1\n",
    "\n",
    "       CONTRAT : le trie des entrées est à la charge des utilisateurs,\n",
    "                 le comportement n'est pas garanti sinon\"\"\"\n",
    "\n",
    "    res = []\n",
    "    n1 = len(table1)\n",
    "    n2 = len(table2)\n",
    "    ind1 = ind2 = 0\n",
    "    while ind1 < n1 and ind2 < n2:\n",
    "        t1, t2 = table1[ind1], table2[ind2]\n",
    "        if t1[attr1] < t2[attr2]:\n",
    "            ind1 += 1\n",
    "        elif t1[attr1] > t2[attr2]:\n",
    "            ind2 += 1\n",
    "        else:\n",
    "            ind3 = ind2\n",
    "            while ind3 < n2 and table2[ind3][attr2] == t1[attr1]:\n",
    "                res.append(t1 + table2[ind3])\n",
    "                ind3 += 1\n",
    "            ind1 += 1\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "fjunier@fjunier:~/Git/DIU-Junier/bloc4/BDD_Conception_Normalisation/sandbox$ pytest-3 -v\n",
    "============================= test session starts ==============================\n",
    "platform linux -- Python 3.6.9, pytest-3.3.2, py-1.5.2, pluggy-0.6.0 -- /usr/bin/python3\n",
    "cachedir: .cache\n",
    "rootdir: /home/fjunier/Git/DIU-Junier/bloc4/BDD_Conception_Normalisation/sandbox, inifile:\n",
    "plugins: Faker-4.1.0\n",
    "collected 3 items\n",
    "\n",
    "join_algorithms_test.py::test_join_nested_loop PASSED                    [ 33%]\n",
    "join_algorithms_test.py::test_join_hash PASSED                           [ 66%]\n",
    "join_algorithms_test.py::test_join_merge PASSED                          [100%]\n",
    "\n",
    "=========================== 3 passed in 0.07 seconds ===========================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Comparer algorithmes implantés en Python\n",
    "----------------------------------------\n",
    "\n",
    "On peut maintenant comparer la performance des algorithmes avec la fonction fournie `benchmark`.Pour la fonction `join_merge` on compte séparément le temps pris pour le tris des tables.\n",
    "En effet, cette étape peut-être _amortie_ car elle est utile pour d'autre opérations que la jointure, comme les clauses `ORDER BY` ou `GROUP BY`.\n",
    "\n",
    "**EXERCICE** : comprendre ce que fait la fonction `benchmark` (vous pouvez ajouter des commentaires par exempl)e avant de l'exécuter. \n",
    "\n",
    "Avec les paramètres par défaut de `benchmark`, on obtient les résultats suivants sur une machine portable (Dual Core Intel i7-5600U CPU @ 2.60GHz, 8GB RAM).\n",
    "\n",
    "```\n",
    "Temps pour une exécution de join_nested_loop : 4.8441446340002585\n",
    "Temps pour une exécution de join_hash        : 0.1884105869976338\n",
    "Temps pour une exécution des tris            : 0.018489982991013676\n",
    "Temps pour une exécution de join_merge       : 0.3174076739960583\n",
    "```\n",
    "\n",
    "**EXERCICE (POUR ALLER PLUS LOIN)** : jouer avec les paramètres pour trouver un cas qui soit défavorable à `join_hash` mais favorable à `join_merge`. Sans tenir compte du temps de tri, on peut trouver des cas avec un facteur 10x en faveur de `join_merge`. _Indice_ : remarquez que les rôles de `table1` et `table2` sont asymétriques faire en sorte de passer du temps dans l'étape de construction d'index de `join_hash`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(size_1=1000, nb_val_1=100,\n",
    "              size_2=1000, nb_val_2=100,\n",
    "              nb_repeat=100,\n",
    "              bench_loop=True, bench_hash=True, bench_merge=True):\n",
    "    \"\"\"Compare les différentes implémentations\"\"\"\n",
    "\n",
    "    #échantillon 1  \n",
    "    sample_1 = [(i, randrange(nb_val_1)) for i in range(size_1)]\n",
    "    #échantillon 2\n",
    "    sample_2 = [(randrange(nb_val_2), 'A'+str(j)) for j in range(size_2)]\n",
    "\n",
    "    if bench_loop:\n",
    "        #temps d'exécution moyen sur nb_repeat tours de boucles \n",
    "        #pour l'exécution de la jointure avec join_nested_loop\n",
    "        #sur la correspondance des attibuts d'index 1 de sample_1 et 0 de sample_2 \n",
    "        #le domaine de sample_1[0] est  randrange(nb_val_1)\n",
    "        #le domaine de sample_2[0] est  randrange(nb_val_2)\n",
    "        time_loop = timeit(lambda: join_nested_loop(sample_1, 1, sample_2, 0), number=nb_repeat)\n",
    "        print('Temps pour une exécution de join_nested_loop : ' + str(time_loop))\n",
    "\n",
    "    if bench_hash:\n",
    "        #temps d'exécution moyen sur nb_repeat tours de boucles \n",
    "        #pour l'exécution de la jointure avec join_hash\n",
    "        time_hash = timeit(lambda: join_hash(sample_1, 1, sample_2, 0), number=nb_repeat)\n",
    "        print('Temps pour une exécution de join_hash        : ' + str(time_hash))\n",
    "\n",
    "    if bench_merge:\n",
    "        #temps d'exécution du tri de sample_1\n",
    "        time_sort = timeit(lambda: sample_1.sort(key=lambda x: x[1]), number=nb_repeat)\n",
    "        #temps d'exécution du tri de sample_2\n",
    "        time_sort += timeit(lambda: sample_2.sort(key=lambda x: x[0]), number=nb_repeat)\n",
    "        #temps d'exécution moyen sur nb_repeat tours de boucles \n",
    "        #pour l'exécution de la jointure avec join_merge\n",
    "        time_merge = timeit(lambda: join_merge(sample_1, 1, sample_2, 0), number=nb_repeat)\n",
    "        print('Temps pour une exécution des tris            : ' + str(time_sort))\n",
    "        print('Temps pour une exécution de join_merge       : ' + str(time_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour une exécution de join_nested_loop : 6.347709445999499\n",
      "Temps pour une exécution de join_hash        : 0.20662388300115708\n",
      "Temps pour une exécution des tris            : 0.022455870999692706\n",
      "Temps pour une exécution de join_merge       : 0.3163491819977935\n"
     ]
    }
   ],
   "source": [
    "benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Test avec join_hash première version\n",
    "In [2]: benchmark()\n",
    "Temps pour une exécution de join_nested_loop : 6.241203233000078\n",
    "Temps pour une exécution de join_hash        : 0.16518595900015498\n",
    "Temps pour une exécution des tris            : 0.020488148000140427\n",
    "Temps pour une exécution de join_merge       : 0.3143313499999749\n",
    "```\n",
    "\n",
    "```\n",
    "Test avec join_hash seconde version\n",
    "In [4]: benchmark()\n",
    "Temps pour une exécution de join_nested_loop : 6.2457434129999\n",
    "Temps pour une exécution de join_hash        : 0.162923426000134\n",
    "Temps pour une exécution des tris            : 0.02036094400023103\n",
    "Temps pour une exécution de join_merge       : 0.2916302970002107\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de complexité \n",
    "\n",
    "Soit `m` le nombre de lignes dans la jointure. Ce nombre est majoré par `max(nb_val_1, nb_val_2)`\n",
    "\n",
    "* Complexité de `join_nested_loop` : `O(size_1 * size_2)`\n",
    "* Complexité de `join_hash` :  `O(size_1 * n + size_2 * n)` (majoration grossière) où `n` est le nombre maximal de tuple de l'échantillon `sample_1` qui ont la même valeur sur l'attribut 1 (longueur maximale d'une liste d'indexs stockée dans l'index/table de hash) : la valeur de `n` est d'autant plus grande que le domaine de `sample_1[1]` de taille `nb_val_1` est grand. On peut donc prendre comme majorant `O(size_1 * nb_val_1 + size_2 * nb_val_1)` \n",
    "* Complexité de `join_merge` (sans compter le tri des tables) :  `O(size_1 + size_2 + size_2 * m)` où `m` est le nombre de lignes dans la jointure.\n",
    "\n",
    "\n",
    "Si `size_1 == size_2` et `nb_val_1 == nb_val_2`, comme les valeurs sont choisies selon une loi uniforme dans `[0;nb_val_1]`,   les complexités de `join_hash` et `join_merge` doivent avoir  des majorants du même ordre de grandeur, qui vont différer par une constante. On remarque que `join_hash` est plus rapide : les tests d'égalité sont remplacés par des calculs de `hash` dans la phase de construction de l'index puis dans la phase de scan de `sample_2`.\n",
    "\n",
    "Si on augmente la taille de `nb_val_1`, on augmente la valeur  de `n`  le nombre maximal de tuple de l'échantillon `sample_1` qui ont la même valeur sur l'attribut 1 et la complexité de `join_hash` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_1 = 100, nb_val_1=100, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0007095030014170334\n",
      "Temps pour une exécution des tris            : 0.0002691189984034281\n",
      "Temps pour une exécution de join_merge       : 0.0008670200004416984\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.00036865099900751375\n",
      "Temps pour une exécution des tris            : 0.00026027300191344693\n",
      "Temps pour une exécution de join_merge       : 0.0002898899983847514\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0003533569979481399\n",
      "Temps pour une exécution des tris            : 0.0002776670007733628\n",
      "Temps pour une exécution de join_merge       : 0.0002584450012363959\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.00035543699777917936\n",
      "Temps pour une exécution des tris            : 0.0002571869954408612\n",
      "Temps pour une exécution de join_merge       : 0.00024345500060007907\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0003277340001659468\n",
      "Temps pour une exécution des tris            : 0.00027071399745182134\n",
      "Temps pour une exécution de join_merge       : 0.0002511099992261734\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.005385041000408819\n",
      "Temps pour une exécution des tris            : 0.0016174449992831796\n",
      "Temps pour une exécution de join_merge       : 0.0063748339998710435\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0034886419998656493\n",
      "Temps pour une exécution des tris            : 0.0014653040016128216\n",
      "Temps pour une exécution de join_merge       : 0.0007519009996030945\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.004028418003144907\n",
      "Temps pour une exécution des tris            : 0.0015763789997436106\n",
      "Temps pour une exécution de join_merge       : 0.00031256999864126556\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0028217020008014515\n",
      "Temps pour une exécution des tris            : 0.0014113149991317187\n",
      "Temps pour une exécution de join_merge       : 0.0002673420021892525\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0027006780001102015\n",
      "Temps pour une exécution des tris            : 0.0014165499960654415\n",
      "Temps pour une exécution de join_merge       : 0.00026495000201975927\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.050292395000724355\n",
      "Temps pour une exécution des tris            : 0.01493770399974892\n",
      "Temps pour une exécution de join_merge       : 0.06040234399915789\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.03877774400098133\n",
      "Temps pour une exécution des tris            : 0.015654769998945994\n",
      "Temps pour une exécution de join_merge       : 0.006641958003456239\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.044798663999245036\n",
      "Temps pour une exécution des tris            : 0.015170044003752992\n",
      "Temps pour une exécution de join_merge       : 0.0010511910004424863\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.027671412000927376\n",
      "Temps pour une exécution des tris            : 0.014017621000675717\n",
      "Temps pour une exécution de join_merge       : 0.00031668500014347956\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 100,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.02787800100122695\n",
      "Temps pour une exécution des tris            : 0.013883913994504837\n",
      "Temps pour une exécution de join_merge       : 0.00022690699915983714\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.00035988200033898465\n",
      "Temps pour une exécution des tris            : 0.0002421079989289865\n",
      "Temps pour une exécution de join_merge       : 0.00020964499708497897\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.00039180199746624567\n",
      "Temps pour une exécution des tris            : 0.0002473579988873098\n",
      "Temps pour une exécution de join_merge       : 0.0004876949969911948\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.00033853900094982237\n",
      "Temps pour une exécution des tris            : 0.00026692500250646845\n",
      "Temps pour une exécution de join_merge       : 0.00028329199994914234\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0003159949992550537\n",
      "Temps pour une exécution des tris            : 0.0002656400029081851\n",
      "Temps pour une exécution de join_merge       : 0.00023448100182577036\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.00031866200151853263\n",
      "Temps pour une exécution des tris            : 0.0002471230036462657\n",
      "Temps pour une exécution de join_merge       : 0.00023484399935114197\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0031936190025589895\n",
      "Temps pour une exécution des tris            : 0.001331611998466542\n",
      "Temps pour une exécution de join_merge       : 0.0024420039990218356\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0033456829987699166\n",
      "Temps pour une exécution des tris            : 0.0013962269986222964\n",
      "Temps pour une exécution de join_merge       : 0.002632111998536857\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.002878084000258241\n",
      "Temps pour une exécution des tris            : 0.0013731479994021356\n",
      "Temps pour une exécution de join_merge       : 0.0005597939998551738\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.002671294001629576\n",
      "Temps pour une exécution des tris            : 0.0013584270018327516\n",
      "Temps pour une exécution de join_merge       : 0.00025796799673116766\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.002631456001836341\n",
      "Temps pour une exécution des tris            : 0.0013347599997359794\n",
      "Temps pour une exécution de join_merge       : 0.000231142999837175\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.03578000500056078\n",
      "Temps pour une exécution des tris            : 0.01933056899724761\n",
      "Temps pour une exécution de join_merge       : 0.030681606996949995\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.05209140500301146\n",
      "Temps pour une exécution des tris            : 0.020404863000294426\n",
      "Temps pour une exécution de join_merge       : 0.03566601400234504\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.05447609899783856\n",
      "Temps pour une exécution des tris            : 0.02204891399742337\n",
      "Temps pour une exécution de join_merge       : 0.0026955369976349175\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.03584148500158335\n",
      "Temps pour une exécution des tris            : 0.014716902001964627\n",
      "Temps pour une exécution de join_merge       : 0.00046618500346085057\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 100,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.026724644001660636\n",
      "Temps pour une exécution des tris            : 0.014154301996313734\n",
      "Temps pour une exécution de join_merge       : 0.0002768079975794535\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.004489417999138823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour une exécution des tris            : 0.0012890119978692383\n",
      "Temps pour une exécution de join_merge       : 0.0058464280009502545\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0020343610012787394\n",
      "Temps pour une exécution des tris            : 0.0018300260016985703\n",
      "Temps pour une exécution de join_merge       : 0.0036871280026389286\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.001456025998777477\n",
      "Temps pour une exécution des tris            : 0.0017990590022236574\n",
      "Temps pour une exécution de join_merge       : 0.003369305999513017\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0008365869980480056\n",
      "Temps pour une exécution des tris            : 0.0013364509977691341\n",
      "Temps pour une exécution de join_merge       : 0.0024299129981955048\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0009031779991346411\n",
      "Temps pour une exécution des tris            : 0.0013975630026834551\n",
      "Temps pour une exécution de join_merge       : 0.0024079220020212233\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.02178069699948537\n",
      "Temps pour une exécution des tris            : 0.0024614090034447145\n",
      "Temps pour une exécution de join_merge       : 0.03092013599962229\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.006579272998351371\n",
      "Temps pour une exécution des tris            : 0.0024091620034596417\n",
      "Temps pour une exécution de join_merge       : 0.004844920000323327\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0039735329992254265\n",
      "Temps pour une exécution des tris            : 0.0024837529999786057\n",
      "Temps pour une exécution de join_merge       : 0.0036209370009601116\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.003805101998295868\n",
      "Temps pour une exécution des tris            : 0.0024769029987510294\n",
      "Temps pour une exécution de join_merge       : 0.0025558620000083465\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.003904576002241811\n",
      "Temps pour une exécution des tris            : 0.0024669419981364626\n",
      "Temps pour une exécution de join_merge       : 0.002709286000026623\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.29143684600057895\n",
      "Temps pour une exécution des tris            : 0.014275331999670016\n",
      "Temps pour une exécution de join_merge       : 0.3137324559975241\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.05207130700000562\n",
      "Temps pour une exécution des tris            : 0.015715824003564194\n",
      "Temps pour une exécution de join_merge       : 0.03659084399987478\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.051723884000239195\n",
      "Temps pour une exécution des tris            : 0.020580874996085186\n",
      "Temps pour une exécution de join_merge       : 0.005231080001976807\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.04037144700123463\n",
      "Temps pour une exécution des tris            : 0.01569839700096054\n",
      "Temps pour une exécution de join_merge       : 0.0027995540003757924\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 1000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.02719253900067997\n",
      "Temps pour une exécution des tris            : 0.01683989999946789\n",
      "Temps pour une exécution de join_merge       : 0.003742057000636123\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0012857559995609336\n",
      "Temps pour une exécution des tris            : 0.0014191269983712118\n",
      "Temps pour une exécution de join_merge       : 0.0008375430006708484\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0012907259988423903\n",
      "Temps pour une exécution des tris            : 0.0013451980012177955\n",
      "Temps pour une exécution de join_merge       : 0.003015392998349853\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0009615660019335337\n",
      "Temps pour une exécution des tris            : 0.0014088090028963052\n",
      "Temps pour une exécution de join_merge       : 0.002508640998712508\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0008423399995081127\n",
      "Temps pour une exécution des tris            : 0.001355889002297772\n",
      "Temps pour une exécution de join_merge       : 0.0023520680006186012\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0008357959995919373\n",
      "Temps pour une exécution des tris            : 0.0014030189995537512\n",
      "Temps pour une exécution de join_merge       : 0.0023920579988043755\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.006164253998576896\n",
      "Temps pour une exécution des tris            : 0.002401944002485834\n",
      "Temps pour une exécution de join_merge       : 0.005775875000836095\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.00730552099776105\n",
      "Temps pour une exécution des tris            : 0.002459792998706689\n",
      "Temps pour une exécution de join_merge       : 0.008017023999855155\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.004402835002110805\n",
      "Temps pour une exécution des tris            : 0.00246723199961707\n",
      "Temps pour une exécution de join_merge       : 0.003030103998753475\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0034449410013621673\n",
      "Temps pour une exécution des tris            : 0.002589463001640979\n",
      "Temps pour une exécution de join_merge       : 0.003100028003245825\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.0034106679995602462\n",
      "Temps pour une exécution des tris            : 0.0024727869968046434\n",
      "Temps pour une exécution de join_merge       : 0.0023646080007893033\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.048668284001905704\n",
      "Temps pour une exécution des tris            : 0.01361332700253115\n",
      "Temps pour une exécution de join_merge       : 0.0546319869972649\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.05216387000109535\n",
      "Temps pour une exécution des tris            : 0.015095377999386983\n",
      "Temps pour une exécution de join_merge       : 0.06466487499710638\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.03692742199928034\n",
      "Temps pour une exécution des tris            : 0.015517948999331566\n",
      "Temps pour une exécution de join_merge       : 0.008153801001753891\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.028756618001352763\n",
      "Temps pour une exécution des tris            : 0.015147648002312053\n",
      "Temps pour une exécution de join_merge       : 0.0028918709977006074\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 1000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.028260918999876594\n",
      "Temps pour une exécution des tris            : 0.015787178999744356\n",
      "Temps pour une exécution de join_merge       : 0.0026626670005498454\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 10000,  nb_val_2=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour une exécution de join_hash        : 0.046720335998543305\n",
      "Temps pour une exécution des tris            : 0.015161491999606369\n",
      "Temps pour une exécution de join_merge       : 0.055268427000555675\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.011525769001309527\n",
      "Temps pour une exécution des tris            : 0.01236080400121864\n",
      "Temps pour une exécution de join_merge       : 0.0278330460023426\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0065579709989833646\n",
      "Temps pour une exécution des tris            : 0.012649822001549182\n",
      "Temps pour une exécution de join_merge       : 0.024370677998376777\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.006019319000188261\n",
      "Temps pour une exécution des tris            : 0.012272661999304546\n",
      "Temps pour une exécution de join_merge       : 0.023959483001817716\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.00817941700006486\n",
      "Temps pour une exécution des tris            : 0.012882245999207953\n",
      "Temps pour une exécution de join_merge       : 0.024064730998361483\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.23456956900190562\n",
      "Temps pour une exécution des tris            : 0.014187753000442171\n",
      "Temps pour une exécution de join_merge       : 0.30957386400041287\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.03737551199810696\n",
      "Temps pour une exécution des tris            : 0.013795012997434242\n",
      "Temps pour une exécution de join_merge       : 0.04713717899721814\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.024481184002070222\n",
      "Temps pour une exécution des tris            : 0.01429301700409269\n",
      "Temps pour une exécution de join_merge       : 0.030016351000085706\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.008890439999959199\n",
      "Temps pour une exécution des tris            : 0.013811185999657027\n",
      "Temps pour une exécution de join_merge       : 0.025843174000328872\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.0086988110015227\n",
      "Temps pour une exécution des tris            : 0.013808662002702476\n",
      "Temps pour une exécution de join_merge       : 0.023776517999067437\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 2.257922086999315\n",
      "Temps pour une exécution des tris            : 0.02489026400144212\n",
      "Temps pour une exécution de join_merge       : 3.017150016999949\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.2912402949987154\n",
      "Temps pour une exécution des tris            : 0.027682451000146102\n",
      "Temps pour une exécution de join_merge       : 0.3594973450017278\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.08776485799899092\n",
      "Temps pour une exécution des tris            : 0.032489858003827976\n",
      "Temps pour une exécution de join_merge       : 0.060453613001300255\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.05483210199963651\n",
      "Temps pour une exécution des tris            : 0.026883465998253087\n",
      "Temps pour une exécution de join_merge       : 0.033962191999307834\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 10000,  nb_val_2=100\n",
      "Temps pour une exécution de join_hash        : 0.04519766399971559\n",
      "Temps pour une exécution des tris            : 0.02768325099896174\n",
      "Temps pour une exécution de join_merge       : 0.02606482400005916\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.01066436499968404\n",
      "Temps pour une exécution des tris            : 0.01451446700230008\n",
      "Temps pour une exécution de join_merge       : 0.006249242000194499\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.014869651997287292\n",
      "Temps pour une exécution des tris            : 0.014843921999272425\n",
      "Temps pour une exécution de join_merge       : 0.044196758997713914\n",
      "----------\n",
      "size_1 = 100, nb_val_1=10000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.006712914000672754\n",
      "Temps pour une exécution des tris            : 0.014816891998634674\n",
      "Temps pour une exécution de join_merge       : 0.03147936199820833\n",
      "----------\n",
      "size_1 = 100, nb_val_1=100000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.006342584998492384\n",
      "Temps pour une exécution des tris            : 0.013999775004776893\n",
      "Temps pour une exécution de join_merge       : 0.02518306400088477\n",
      "----------\n",
      "size_1 = 100, nb_val_1=1000000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.006532302999403328\n",
      "Temps pour une exécution des tris            : 0.016020556999137625\n",
      "Temps pour une exécution de join_merge       : 0.02475426199816866\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.025126655000349274\n",
      "Temps pour une exécution des tris            : 0.014367476003826596\n",
      "Temps pour une exécution de join_merge       : 0.02786236999963876\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.04199574400263373\n",
      "Temps pour une exécution des tris            : 0.014480354002444074\n",
      "Temps pour une exécution de join_merge       : 0.05314602199723595\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=10000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.014797607000218704\n",
      "Temps pour une exécution des tris            : 0.0145864040023298\n",
      "Temps pour une exécution de join_merge       : 0.02785071000107564\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=100000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.010014903997216607\n",
      "Temps pour une exécution des tris            : 0.025981351001973962\n",
      "Temps pour une exécution de join_merge       : 0.03192392099663266\n",
      "----------\n",
      "size_1 = 1000, nb_val_1=1000000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.011576154000067618\n",
      "Temps pour une exécution des tris            : 0.019135770999128\n",
      "Temps pour une exécution de join_merge       : 0.028446469001210062\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.25315583599876845\n",
      "Temps pour une exécution des tris            : 0.028742687001795275\n",
      "Temps pour une exécution de join_merge       : 0.3527721630016458\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.3220981349986687\n",
      "Temps pour une exécution des tris            : 0.03169777299990528\n",
      "Temps pour une exécution de join_merge       : 0.36848581899903365\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=10000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.08324161299970001\n",
      "Temps pour une exécution des tris            : 0.027340351000020746\n",
      "Temps pour une exécution de join_merge       : 0.05346426799951587\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=100000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.047667820002970984\n",
      "Temps pour une exécution des tris            : 0.027411724000558024\n",
      "Temps pour une exécution de join_merge       : 0.026895713999692816\n",
      "----------\n",
      "size_1 = 10000, nb_val_1=1000000, size_2 = 10000,  nb_val_2=1000\n",
      "Temps pour une exécution de join_hash        : 0.03525492900007521\n",
      "Temps pour une exécution des tris            : 0.026828274996660184\n",
      "Temps pour une exécution de join_merge       : 0.024807427998894127\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for size_2 in [10**k for k in range(2, 5)]:    \n",
    "    for nb_val_2 in [10**k for k in range(2, 4)]:\n",
    "        for size_1 in [10**k for k in range(2, 5)]: \n",
    "            for nb_val_1 in [10**k for k in range(2, 7)]:\n",
    "                print(f\"size_1 = {size_1}, nb_val_1={nb_val_1}, size_2 = {size_2},  nb_val_2={nb_val_2}\")\n",
    "                benchmark(bench_loop=False, size_1 = size_1, nb_val_1 =nb_val_1  ,size_2 = size_2, nb_val_2 = nb_val_2, nb_repeat = 10)\n",
    "                print('-'* 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que `join_merge` est beaucoup plus rapide lorsque `size_1` est beaucoup plus grand que `size_2` et d'autant plus que `nb_val_1` est grand (et donc la longueur maximale d'une liste stockée dans l'index/table de hash)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparer l'exécution dans Python à celle native dans SQLite\n",
    "-------------------------------------------------------------\n",
    "\n",
    "Maintenant, on va comparer la performance de ces implantations Python face aux algorithmes jointures de SQLite (qui est écrit en C). Pour cela on va comparer les deux approches suivantes :\n",
    "\n",
    "* **Approche A : jointure en SQLite**, on exécute la requête `SELECT * FROM table1 JOIN table2 ON table1.val == table2.val` puis (depuis Python) on récupère l'intégralité du résultat, c'est la fonction `join_python()`\n",
    "* **Approche B : jointure en Python**, on exécute la requête `SELECT * FROM table1` et on stocke son résultat dans un tableau, de même pour `SELECT * FROM table2` puis on utilise un des algorithmes précedents pour faire le calcul de jointure et enfin on renvoie le résultat, c'est la fonction `join_sqlite()`\n",
    "\n",
    "\n",
    "**EXERCICE** : créer une nouvelle base de données nommée `join_algorithms_versus_sqlite3.db` et exécuter le script SQL `join_algorithms_schema.sql` pour créer le schéma *et* peupler la base avec un jeu de données similaire à celui du benchmark de l'exercice précédent.\n",
    "\n",
    "**EXERCICE** : en vous inspirant du code fourni, compléter les fonctions `join_python()` et `join_sqlite()` du programme [`join_algorithms_versus_sqlite3.py`](join_algorithms_versus_sqlite3.py) et observer les temps d'exécution.\n",
    "\n",
    " **EXERCICE** : ensuite, exécuter la requête de jointure directement dans `SQLite3` en activant activant le chronométrage avec `.timer on` depuis l'interpréteur ligne de commande ou depuis _DB Browser for SQLite_ (le temps est affiché en bas de la fenêtre d'exécution). Vous devriez avoir une différence _de plusieurs ordre de magnitude entre les deux_ : comment l'expliquer ?\n",
    "\n",
    " **EXERCICE** : reprendre la comparaison mais cette fois avec la requête `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val`. Ici, `join_python()` renverra _la longueur du tableau_, comme par exemple `len(join_hash(table1, 1, table2, 0))` pour l'algorithme de jointure par hash. Comparer les temps d'exécution, une différence _de plusieurs ordre de magnitude doit les séparer_  : comment l'expliquer ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%config SqlMagic.displaycon = False\n",
    "%config SqlMagic.autolimit = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql sqlite:///join_algorithms_versus_sqlite3.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "PRAGMA foreign_keys=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "-- POUR CHARGER DANS SQLite3 : \n",
    "-- sqlite3 join_algorithms_versus_sqlite3.db\n",
    "-- .timer on\n",
    "-- .read join_algorithms_schema.sql\n",
    "\n",
    "drop table if exists Table1;\n",
    "drop table if exists Table2;\n",
    "\n",
    "-- Un schéma très simple, qui reprend le benchmark des 3 algos en Python qu'on rappelle ici :\n",
    "--\n",
    "-- def benchmark(size_1=1000, nb_val_1=100,\n",
    "--               size_2=1000, nb_val_2=100,\n",
    "--               nb_repeat=100,\n",
    "--               bench_loop=True, bench_hash=True, bench_merge=True):\n",
    "--\n",
    "--    sample_1 = [(i, randrange(nb_val_1)) for i in range(size_1)]\n",
    "--    sample_2 = [(randrange(nb_val_2), 'A'+str(j)) for j in range(size_2)]\n",
    "\n",
    "-- Activation des clefs étrangères\n",
    "PRAGMA foreign_keys=1; \n",
    "\n",
    "create table Table1(\n",
    "    idA INTEGER PRIMARY KEY,    -- un ID entier\n",
    "    val INTEGER     -- l'attribut de jointure\n",
    ");\n",
    "\n",
    "create table Table2(\n",
    "    val INTEGER,  -- pas  REFERENCES Table1(val), mais ca ne changerait rien\n",
    "    idB INTEGER\n",
    ");\n",
    "\n",
    "\n",
    " -- On va remplir les tables avec 10.000 lignes pour A et autant pour B\n",
    "\n",
    "--pour générer récursivement tous les entiers entre 1 et 10000\n",
    "--voir https://sqlpro.developpez.com/cours/sqlserver/cte-recursives/\n",
    "WITH RECURSIVE data_1(val) AS (\n",
    "  SELECT 1\n",
    "  UNION ALL\n",
    "  SELECT val+1\n",
    "  FROM data_1\n",
    "  WHERE val+1 <= 10000\n",
    ") \n",
    "INSERT INTO Table1 \n",
    "  SELECT  val,\n",
    "          ABS(RANDOM() % 1000)   --entiers aléatoires entre 0 et 1000\n",
    "  FROM data_1;\n",
    "\n",
    "WITH RECURSIVE data_2(val) AS (\n",
    "  SELECT 1\n",
    "  UNION ALL\n",
    "  SELECT val+1\n",
    "  FROM data_2\n",
    "  WHERE val+1 <= 10000\n",
    ") \n",
    "INSERT INTO Table2\n",
    "  SELECT  ABS(RANDOM() % 1000),\n",
    "          'A' || val\n",
    "  FROM data_2;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Un module pour comparer la performance des jointures 'à la main'\n",
    "    en Python face à ceux en C bien choisis de SQLite3\"\"\"\n",
    "\n",
    "import sqlite3\n",
    "import logging\n",
    "#voir https://docs.python.org/3/howto/logging.html\n",
    "from timeit import timeit\n",
    "#from join_algorithms import join_nested_loop, join_hash, join_merge\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "DB_FILE = 'join_algorithms_versus_sqlite3.db'\n",
    "\n",
    "def join_algorithms_versus_sqlite3():\n",
    "    \"\"\"Fonction de comparaison\"\"\"\n",
    "    try:\n",
    "        connection = sqlite3.connect(DB_FILE)\n",
    "\n",
    "        # Pour avoir des ditcionnaires et non des tuples dans le résultat\n",
    "        # see https://docs.python.org/3.6/library/sqlite3.html#sqlite3.Row\n",
    "        # connection.row_factory = sqlite3.Row\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        logging.debug(\"You are connected to - %s\", DB_FILE)\n",
    "        cursor.execute(\"SELECT sqlite_version() as version;\")\n",
    "        record = cursor.fetchone()\n",
    "        logging.debug(tuple(record))\n",
    "\n",
    "        def join_python(jointure):\n",
    "            cursor.execute(\"SELECT * FROM table1;\")\n",
    "            table1 = cursor.fetchall()\n",
    "            #print(table1[:10])\n",
    "            cursor.execute(\"SELECT * FROM table2;\")\n",
    "            table2 = cursor.fetchall()\n",
    "            jointure(table1, 1,table2, 0)\n",
    "            \n",
    "        def join_sqlite():\n",
    "            cursor.execute(\"SELECT * FROM table1 JOIN table2 ON table1.val == table2.val;\")\n",
    "            record = cursor.fetchone()\n",
    "            \n",
    "        for jointure in [join_hash, join_merge]:\n",
    "            time_join_python = timeit(lambda : join_python(jointure), number=100)\n",
    "            logging.info(f'Temps pour une jointure {jointure.__name__} côté Python : %f', time_join_python)\n",
    "\n",
    "        time_join_sqlite = timeit(join_sqlite, number=100)\n",
    "        logging.info('Temps pour une jointure côté Sqlite3 : %f', time_join_sqlite)\n",
    "\n",
    "    except (sqlite3.Error) as error:\n",
    "        logging.error(\"Error while connecting to sqlite3: %s\", error)\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            logging.debug(\"Sqlite3 connection is closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Temps pour une jointure join_hash côté Python : 4.699353\n",
      "INFO:root:Temps pour une jointure join_merge côté Python : 1.603253\n",
      "INFO:root:Temps pour une jointure côté Sqlite3 : 0.854773\n"
     ]
    }
   ],
   "source": [
    "join_algorithms_versus_sqlite3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **EXERCICE** : ensuite, exécuter la requête de jointure directement dans `SQLite3` en activant activant le chronométrage avec `.timer on` depuis l'interpréteur ligne de commande ou depuis _DB Browser for SQLite_ (le temps est affiché en bas de la fenêtre d'exécution). Vous devriez avoir une différence _de plusieurs ordre de magnitude entre les deux_ : comment l'expliquer ?\n",
    " \n",
    "* sqlite3 CLI :\n",
    "\n",
    "![sqlite3-CLI](time_sqlite3.png)\n",
    "\n",
    "* sqlite DBBrowser :\n",
    "\n",
    "![sqlite DBBrowser](time_dbbrowser.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCICE** : reprendre la comparaison mais cette fois avec la requête `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val`. Ici, `join_python()` renverra _la longueur du tableau_, comme par exemple `len(join_hash(table1, 1, table2, 0))` pour l'algorithme de jointure par hash. Comparer les temps d'exécution, une différence _de plusieurs ordre de magnitude doit les séparer_  : comment l'expliquer ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_algorithms_versus_sqlite3_V2():\n",
    "    \"\"\"Fonction de comparaison\"\"\"\n",
    "    try:\n",
    "        connection = sqlite3.connect(DB_FILE)\n",
    "\n",
    "        # Pour avoir des ditcionnaires et non des tuples dans le résultat\n",
    "        # see https://docs.python.org/3.6/library/sqlite3.html#sqlite3.Row\n",
    "        # connection.row_factory = sqlite3.Row\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        logging.debug(\"You are connected to - %s\", DB_FILE)\n",
    "        cursor.execute(\"SELECT sqlite_version() as version;\")\n",
    "        record = cursor.fetchone()\n",
    "        logging.debug(tuple(record))\n",
    "\n",
    "        def join_python(jointure):\n",
    "            cursor.execute(\"SELECT * FROM table1;\")\n",
    "            table1 = cursor.fetchall()\n",
    "            #print(table1[:10])\n",
    "            cursor.execute(\"SELECT * FROM table2;\")\n",
    "            table2 = cursor.fetchall()           \n",
    "            return len( jointure(table1, 1,table2, 0))\n",
    "            \n",
    "        def join_sqlite():\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val;\")\n",
    "            record = cursor.fetchone()\n",
    "            return record\n",
    "        \n",
    "                \n",
    "        for jointure in [join_hash, join_merge]:\n",
    "            time_join_python = timeit(lambda : join_python(jointure), number=100)\n",
    "            logging.info(f'Temps pour une jointure {jointure.__name__} côté Python : %f', time_join_python)\n",
    "            \n",
    "        time_join_sqlite = timeit(join_sqlite, number=100)\n",
    "        logging.info('Temps pour une jointure côté Sqlite3 : %f', time_join_sqlite)\n",
    "\n",
    "        \n",
    "\n",
    "    except (sqlite3.Error) as error:\n",
    "        logging.error(\"Error while connecting to sqlite3: %s\", error)\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            logging.debug(\"Sqlite3 connection is closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Temps pour une jointure join_hash côté Python : 4.644944\n",
      "INFO:root:Temps pour une jointure join_merge côté Python : 1.659649\n",
      "INFO:root:Temps pour une jointure côté Sqlite3 : 1.624037\n"
     ]
    }
   ],
   "source": [
    "join_algorithms_versus_sqlite3_V2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas, on a un facteur 100 entre l'exécution en ligne de commandes et l'exécution à travers un programme Python. La requête `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val;` est plus lente que  `SELECT * FROM table1 JOIN table2 ON table1.val == table2.val;` depuis un programme Python si on l'exécute dans `sqlite3` d'abord et similaire si on fait la jointure après avec nos fonctions maisons. On observe au contraire que la  requête  `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val;`  s'exécute plus rapidement en ligne de commande.\n",
    "\n",
    "\n",
    "* sqlite3 CLI :\n",
    "\n",
    "![sqlite3-CLI](timer2_sqlite3.png)\n",
    "\n",
    "* sqlite DBBrowser :\n",
    "\n",
    "![sqlite DBBrowser](timer2_sqliteDBBrowser.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EXERCICE (FACULTATIF ET OUVERT) :__\n",
    "\n",
    "Conclure en formulant quelques bonnes pratiques de l'accès à une base de données via un programme (Python).\n",
    "\n",
    "\n",
    "* Bonne pratique 1 : Faire les opérations SQL (jointures) dans SQLite et non dans Python\n",
    "* Bonne pratique 2 : se méfier des tests en ligne de commandes qui peuvent donner de mauvaises indications de performances (voir exercice précédent avec les requêtes `SELECT * FROM table1 JOIN table2 ON table1.val == table2.val;` et `SELECT COUNT(*) FROM table1 JOIN table2 ON table1.val == table2.val;`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
